---
title: "Generalized Linear Mixed Models for Prevalences"
author: "Coimbra, van der Kuil, van de Schoot"
date: "2025-07-20"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    toc_depth: 4
    number_sections: true
    code_folding: hide
  word_document:
    toc: true
    toc_depth: '4'
  pdf_document:
    toc: true
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = FALSE, cache = FALSE)

# Set the root directory for the entire document (recommended way)
knitr::opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))

```

# Preparation

```{r load-packages}
library(readr)
library(metafor)
library(lme4)
library(influence.ME)
library(dplyr)
library(gt)
```

## Load Data

```{r load-data}
# load data
df_moderation <- read.csv2("../pre-processing/output/data_for_moderation_analyses.csv", dec = ".")

# Display the first rows to check the data
head(df_moderation)
```

## Descriptive Statistics

We provide basic descriptive statistics of the included samples, without excluding any data due to missingness.

```{r descriptives}

# Calculate descriptives
total_number_of_samples <- nrow(df_moderation)
total_sample_size <- sum(df_moderation$Sample_Size, na.rm = TRUE)

# Create a summary table
summary_table <- data.frame(
  Description = c("Number of samples / cohorts", "Total sample size (sum of available Sample_Size)"),
  Value = format(c(total_number_of_samples, total_sample_size), big.mark = ",")
)

# Display as a table
knitr::kable(summary_table, caption = "Descriptive Statistics of the Dataset")

```
### Descriptives Tables for All samples

```{r descriptives-samples-table}
overview <- tibble(
  `Authors (Study)` = df_moderation$Study,
  `Sample Size`     = df_moderation$Sample_Size,
  `Mean Age`        = df_moderation$Mean_age,
  `Percentage Women`= df_moderation$Percentage_women,
  `Assessed Trauma Type` = df_moderation$Assessed_trauma_type,
  `Number of TP assessments`  = df_moderation$TP_assessments,
  `Number of Trajectories Found` = df_moderation$N_trajectories

  ) |> 
  arrange(`Authors (Study)`)

if (knitr::is_html_output() || knitr::is_latex_output()) {
  fmt <- if (knitr::is_latex_output()) "latex" else "html"

  overview %>%
    mutate(
      `Sample Size` = format(`Sample Size`, big.mark = ",", trim = TRUE),
      `Mean Age` = ifelse(is.na(`Mean Age`), NA, sprintf("%.1f", `Mean Age`)),
      `Percentage Women` = ifelse(is.na(`Percentage Women`), NA, sprintf("%.1f%%", `Percentage Women`))
    ) |>
    knitr::kable(format = fmt, caption = "Study Overview", booktabs = TRUE, linesep = "") |>
    kableExtra::kable_styling(full_width = FALSE, position = "left",
                              bootstrap_options = c("striped", "hover", "condensed")) |>
    kableExtra::column_spec(1, bold = TRUE) |>
    kableExtra::scroll_box(height = "470px")
}
```
# Prevalences

## 🚀 Low Symptom Trajectory

### Descriptives

We provide additional descriptives based on the Low trajectory variable, using the full dataset.

```{r Low-descriptives}

# Copy the dataset to avoid overwriting the original
df_low <- df_moderation

# Ensure Low_percentage is numeric
df_low$Low_percentage <- as.numeric(df_low$Low_percentage)


# Calculate Low_n as (percentage / 100) * sample size
df_low$Low_n <- round((df_low$Low_percentage / 100) * df_low$Sample_Size)

# Total number of individuals classified as Low (ignoring missingness in other variables)
total_Low_n <- sum(df_low$Low_n, na.rm = TRUE)

# Number of unique samples (assumes a column Study exists)
unique_Low_samples <- length(unique(df_low$Study[!is.na(df_low$Low_percentage)]))

# Display as a table
additional_info <- data.frame(
  Description = c("Total number of individuals in Low trajectory", "Number of unique samples"),
  Value = format(c(total_Low_n, unique_Low_samples), big.mark = ",")
)

knitr::kable(additional_info, caption = "Additional Descriptives for Low Trajectory Analysis")
```

### Generic GLMM results

We estimate the pooled prevalence of the **Low trajectory** using a Generalized Linear Mixed Model (GLMM).\
We apply a continuity correction of 0.5 individuals for samples reporting exactly 0% or 100% Low cases.\
No data is removed from the dataset; the `rma.glmm()` function handles missingness internally.

```{r Low-glmm}



# Apply continuity correction for 0% and 100%
extreme_cases <- which(
  !is.na(df_low$Low_n) &
  !is.na(df_low$Sample_Size) &
  (df_low$Low_n == 0 | df_low$Low_n == df_low$Sample_Size)
)

df_low$Low_n[extreme_cases] <- df_low$Low_n[extreme_cases] + 0.5
df_low$Sample_Size[extreme_cases] <- df_low$Sample_Size[extreme_cases] + 1

# Enforce xi <= ni - 0.5 to prevent metafor errors
df_low$Low_n <- pmin(df_low$Low_n, df_low$Sample_Size - 0.5)

# Fit GLMM
glmm_Low <- rma.glmm(
  measure = "PLO",
  xi = df_low$Low_n,
  ni = df_low$Sample_Size,
  data = df_low
)

# Extract estimates and convert to proportions
pooled_prev <- plogis(coef(glmm_Low))
ci_lower <- plogis(glmm_Low$ci.lb)
ci_upper <- plogis(glmm_Low$ci.ub)

# Create a summary table
Low_summary <- data.frame(
  Trajectory = "Low (Low Symptom)",
  Pooled_Prevalence = round(pooled_prev, 3),
  CI_Lower = round(ci_lower, 3),
  CI_Upper = round(ci_upper, 3),
  samples_included = glmm_Low$k
)

# Display result
knitr::kable(Low_summary, caption = "Pooled Prevalence Estimate for Low Trajectory (GLMM)")
```

### Sensitivity Analysis 1

Excluding samples with Low trajectory ≥90% of the sample

```{r Low-sensitivity_1}
# Create a copy of the data
df_low_sens1 <- df_low

# Subset: Exclude samples with Low trajectory ≥90% of the sample
df_low_sens1 <- df_low_sens1 %>%
  filter(Low_percentage < 90,
         !is.na(Low_n),
         !is.na(Sample_Size))

# Fit GLMM
glmm_Low_sens1 <- rma.glmm(
  measure = "PLO",
  xi = Low_n,
  ni = Sample_Size,
  data = df_low_sens1
)

# Extract pooled prevalence and CI
sens1_prev <- plogis(coef(glmm_Low_sens1))
sens1_ci_lower <- plogis(glmm_Low_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_Low_sens1$ci.ub)

# Create results table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude ≥90% Low",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  samples_included = glmm_Low_sens1$k
)

# Display table
knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Low Trajectory (Excl. ≥90% Prevalence)")

```

### Sensitivity Analysis 2

Including only large samples (N \> 999)

```{r Low-sensitivity-2}
# Create a copy of the data
df_low_sens2 <- df_low

# Subset: Include only samples with Sample_Size > 999
df_low_sens2 <- df_low_sens2 %>%
  filter(Sample_Size > 999,
         !is.na(Low_n),
         !is.na(Sample_Size))

# Fit GLMM
glmm_Low_sens2 <- rma.glmm(
  measure = "PLO",
  xi = Low_n,
  ni = Sample_Size,
  data = df_low_sens2
)

# Extract pooled prevalence and CI
sens2_prev <- plogis(coef(glmm_Low_sens2))
sens2_ci_lower <- plogis(glmm_Low_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_Low_sens2$ci.ub)

# Create results table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  samples_included = glmm_Low_sens2$k
)

# Display table
knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Low Trajectory (Samples >999)")
```

### Sensitivity Analysis 3

Exclude Influential Samples: We conducted an influence analysis to identify samples with disproportionate impact on the pooled estimate using Cook's distance from a glmer model.

```{r Low-influential}

# Fit a GLMM with glmer (binomial model with random study effect)

model_glmer_Low <- glmer(cbind(round(Low_n), Sample_Size - round(Low_n)) ~ 1 + (1 | Study),
                     data = df_low,
                     family = binomial)

# Run influence analysis by samples
infl <- influence(model_glmer_Low, group = "Study")

# Compute Cook's distance
cooks <- cooks.distance(infl)

# Define rule-of-thumb threshold
threshold <- 4 / length(cooks)

# Plot Cook's distances
p1 <- plot(cooks, type = "h", lwd = 2,
     main = "Cook's Distance per Study",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold, col = "red", lty = 2)


# Identify samples above the threshold
influential_indices <- which(cooks > threshold)
influential_samples <- unique(df_low$Study[influential_indices])

# Display samples
influential_table <- data.frame(
  Study_ID = influential_samples,
  Cooks_Distance = round(cooks[influential_indices], 3)
)

knitr::kable(influential_table, caption = "samples Identified as Influential (Cook's Distance)")

```

We reran the GLMM excluding the identified influential samples.

```{r Low-sens-influential}
# Exclude the influential studies
df_Low_noif <- df_low %>%
  filter(!Study %in% influential_studies)

# Refit GLMM (metafor version)
glmm_Low_noinf <- rma.glmm(
  measure = "PLO",
  xi = df_Low_noif$Low_n,
  ni = df_Low_noif$Sample_Size,
  data = df_Low_noif
)

# Extract estimates
est_infl <- plogis(coef(glmm_Low_noinf))
ci_lower_infl <- plogis(glmm_Low_noinf$ci.lb)
ci_upper_infl <- plogis(glmm_Low_noinf$ci.ub)

# Create summary table
sens_infl_summary <- data.frame(
  Analysis = "Excluding Influential Studies",
  Pooled_Prevalence = round(est_infl, 3),
  CI_Lower = round(ci_lower_infl, 3),
  CI_Upper = round(ci_upper_infl, 3),
  Studies_included = glmm_Low_noinf$k
)

knitr::kable(sens_infl_summary, caption = "Sensitivity Analysis: Low Trajectory (Excluding Influential Studies)")

```

### Use `glmer` estimate

with the glmer\`-based intercept estimate:

```{r Low-glmer-summary}
# Extract fixed effect estimate
est_glmer_Low <- fixef(model_glmer_Low)["(Intercept)"]
se_glmer_Low <- sqrt(vcov(model_glmer_Low)["(Intercept)", "(Intercept)"])

# Compute 95% CI (logit scale)
ci_lower_glmer_Low <- est_glmer_Low - 1.96 * se_glmer_Low
ci_upper_glmer_Low <- est_glmer_Low + 1.96 * se_glmer_Low

n_glmer_samples_Low <- length(ranef(model_glmer_Low)$Study[[1]])

glmer_summary_Low <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_Low), 3),
  CI_Lower = round(plogis(ci_lower_glmer_Low), 3),
  CI_Upper = round(plogis(ci_upper_glmer_Low), 3), 
  samples_included = n_glmer_samples_Low
)

knitr::kable(glmer_summary_Low, caption = "Low Trajectory Estimate from glmer Model")

```

**Note on Number of samples Included** The number of samples included in the models may differ between rma.glmm() and glmer(). This is expected because the two methods handle data slightly differently:

rma.glmm() is designed for meta-analysis and accepts aggregated data (events + sample size). It applies continuity corrections directly and may include more samples when appropriate.

glmer() is a general GLMM function. It uses binomial counts and handles missingness or zero counts differently. samples with no variance or missing outcomes may be excluded automatically.

Both approaches are valid, but they may produce slightly different sample sizes. We report both transparently for completeness.

## 🌿 Decreasing Symptoms Trajectory

We estimate the pooled prevalence of the Decreasing trajectory using a Generalized Linear Mixed Model (GLMM).

### Descriptives

```{r Decreasing-descriptives}

# Copy the data
df_Decreasing <- df_moderation

# Ensure Decreasing_percentage is numeric
df_Decreasing$Decreasing_percentage <- as.numeric(df_Decreasing$Decreasing_percentage)


# Calculate Decreasing_n
df_Decreasing$Decreasing_n <- round((df_Decreasing$Decreasing_percentage / 100) * df_Decreasing$Sample_Size)

# Total number of individuals classified as Decreasing
total_Decreasing_n <- sum(df_Decreasing$Decreasing_n, na.rm = TRUE)

# Number of unique samples with Decreasing data
unique_Decreasing_samples <- length(unique(df_Decreasing$Study[!is.na(df_Decreasing$Decreasing_percentage)]))

# Display as a table
Decreasing_info <- data.frame(
  Description = c("Total number of individuals in Decreasing trajectory", "Number of unique samples with Decreasing data"),
  Value = format(c(total_Decreasing_n, unique_Decreasing_samples), big.mark = ",")
)

knitr::kable(Decreasing_info, caption = "Descriptive Statistics for Decreasing Trajectory")

```

### Generic GLMM

```{r Decreasing-glmm}
### Generic GLMM

# Compute raw number of recoveries BEFORE rounding
df_Decreasing <- df_moderation %>%
  mutate(
    Decreasing_percentage = as.numeric(Decreasing_percentage),
    Decreasing_prop = Decreasing_percentage / 100,
    Decreasing_n_raw = Decreasing_prop * Sample_Size
  )

# Apply continuity correction BEFORE rounding
extreme_cases <- !is.na(df_Decreasing$Decreasing_n_raw) & 
           (df_Decreasing$Decreasing_n_raw == 0 | df_Decreasing$Decreasing_n_raw == df_Decreasing$Sample_Size)

df_Decreasing$Decreasing_n_raw[extreme_cases] <- df_Decreasing$Decreasing_n_raw[extreme_cases] + 0.5
df_Decreasing$Sample_Size[extreme_cases] <- df_Decreasing$Sample_Size[extreme_cases] + 1

df_Decreasing$Decreasing_n_raw <- pmin(df_Decreasing$Decreasing_n_raw, df_Decreasing$Sample_Size - 0.5)


# Round to integer after correction
df_Decreasing$Decreasing_n <- round(df_Decreasing$Decreasing_n_raw)

# Sanity check: raw prevalence
df_Decreasing$Raw_Prevalence <- df_Decreasing$Decreasing_n / df_Decreasing$Sample_Size
summary(df_Decreasing$Raw_Prevalence)



# Fit GLMM
glmm_Decreasing <- rma.glmm(
  measure = "PLO",
  xi = df_Decreasing$Decreasing_n,
  ni = df_Decreasing$Sample_Size,
  data = df_Decreasing
)

# Back-transform
Decreasing_prev <- plogis(coef(glmm_Decreasing))
Decreasing_ci_lower <- plogis(glmm_Decreasing$ci.lb)
Decreasing_ci_upper <- plogis(glmm_Decreasing$ci.ub)

# Output
Decreasing_summary <- data.frame(
  Trajectory = "Decreasing",
  Pooled_Prevalence = round(Decreasing_prev, 3),
  CI_Lower = round(Decreasing_ci_lower, 3),
  CI_Upper = round(Decreasing_ci_upper, 3),
  samples_included = glmm_Decreasing$k
)

knitr::kable(Decreasing_summary, caption = "Pooled Prevalence Estimate for Decreasing Trajectory (GLMM)")

```

### Sensitivity Analysis 1

Exclude samples ≥90% Decreasing Proportions

```{r Decreasing-sens1}
# Compute Decreasing proportion
df_Decreasing$Decreasing_Proportion <- df_Decreasing$Decreasing_n / df_Decreasing$Sample_Size

# Subset: Exclude samples with Decreasing ≥90%
df_Decreasing_sens1 <- df_Decreasing %>%
  filter(Decreasing_Proportion < 0.90)

# Fit GLMM
glmm_Decreasing_sens1 <- rma.glmm(
  measure = "PLO",
  xi = Decreasing_n,
  ni = Sample_Size,
  data = df_Decreasing_sens1
)

# Extract results
sens1_prev <- plogis(coef(glmm_Decreasing_sens1))
sens1_ci_lower <- plogis(glmm_Decreasing_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_Decreasing_sens1$ci.ub)

# Create table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude ≥90% Decreasing",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  samples_included = glmm_Decreasing_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Decreasing Trajectory (Excl. ≥90%)")
```

### Sensitivity Analysis 2

Include Only Samples with N \> 999

```{r Decreasing-sens2}
# Subset: Samples with Sample_Size > 999
df_Decreasing_sens2 <- df_Decreasing %>%
  filter(Sample_Size > 999)

# Fit GLMM
glmm_Decreasing_sens2 <- rma.glmm(
  measure = "PLO",
  xi = Decreasing_n,
  ni = Sample_Size,
  data = df_Decreasing_sens2
)

# Extract results
sens2_prev <- plogis(coef(glmm_Decreasing_sens2))
sens2_ci_lower <- plogis(glmm_Decreasing_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_Decreasing_sens2$ci.ub)

# Create table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  samples_included = glmm_Decreasing_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Decreasing Trajectory (Samples >999)")
```

### Sensitivity Analysis 3

Exclude Influential samples

```{r Decreasing-influential}
# Fit glmer model
model_glmer_Decreasing <- glmer(cbind(round(Decreasing_n), Sample_Size - round(Decreasing_n)) ~ 1 + (1 | Study),
                              data = df_Decreasing, family = binomial)

# Influence analysis
infl_Decreasing <- influence(model_glmer_Decreasing, group = "Study")
cooks_Decreasing <- cooks.distance(infl_Decreasing)

# Define threshold
threshold_Decreasing <- 4 / length(cooks_Decreasing)

# Plot Cook's distances for Decreasing
plot(cooks_Decreasing, type = "h", lwd = 2,
     main = "Cook's Distance per sample (Decreasing)",
     ylab = "Cook's Distance", xlab = "sample Index")
abline(h = threshold_Decreasing, col = "red", lty = 2)


# Identify influential samples
influential_idx <- which(cooks_Decreasing > threshold_Decreasing)
influential_samples_Decreasing <- df_Decreasing$Study[influential_idx]

# Display table
influential_table_Decreasing <- data.frame(
  Study_ID = influential_samples_Decreasing,
  Cooks_Distance = round(cooks_Decreasing[influential_idx], 3)
)

knitr::kable(influential_table_Decreasing, caption = "Influential samples in Decreasing Trajectory (Cook's Distance)")
```

Exclude Influential samples

```{r Decreasing-sens-infl}
# Exclude influential samples
df_Decreasing_noinf <- df_Decreasing %>%
  filter(!Study %in% influential_samples_Decreasing)

# Fit GLMM after exclusion
glmm_Decreasing_noinf <- rma.glmm(
  measure = "PLO",
  xi = Decreasing_n,
  ni = Sample_Size,
  data = df_Decreasing_noinf
)

# Extract results
prev_noinf <- plogis(coef(glmm_Decreasing_noinf))
ci_lower_noinf <- plogis(glmm_Decreasing_noinf$ci.lb)
ci_upper_noinf <- plogis(glmm_Decreasing_noinf$ci.ub)

# Create table
sens_infl_summary <- data.frame(
  Analysis = "Excluding Influential samples",
  Pooled_Prevalence = round(prev_noinf, 3),
  CI_Lower = round(ci_lower_noinf, 3),
  CI_Upper = round(ci_upper_noinf, 3),
  samples_included = glmm_Decreasing_noinf$k
)

knitr::kable(sens_infl_summary, caption = "Sensitivity Analysis: Decreasing Trajectory (Excluding Influential samples)")
```

### Use `glmer` estimate

Use with the glmer\`-based intercept estimate:

```{r Decreasing-glmer-summary}

# Extract fixed effect estimate
est_glmer_Decreasing <- fixef(model_glmer_Decreasing)["(Intercept)"]
se_glmer_Decreasing <- sqrt(vcov(model_glmer_Decreasing)["(Intercept)", "(Intercept)"])

ci_lower_glmer_Decreasing <- est_glmer_Decreasing - 1.96 * se_glmer_Decreasing
ci_upper_glmer_Decreasing <- est_glmer_Decreasing + 1.96 * se_glmer_Decreasing

# Number of samples included in glmer model
n_glmer_samples_Decreasing <- length(ranef(model_glmer_Decreasing)$Study[[1]])

# Back-transform to probability scale
glmer_summary_Decreasing <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_Decreasing), 3),
  CI_Lower = round(plogis(ci_lower_glmer_Decreasing), 3),
  CI_Upper = round(plogis(ci_upper_glmer_Decreasing), 3),
  samples_included = n_glmer_samples_Decreasing
)

knitr::kable(glmer_summary_Decreasing, caption = "Decreasing Trajectory Estimate from glmer Model")
```

## ⚠️ Increasing Symptoms Trajectory

We estimate the pooled prevalence of the Increasing trajectory using a Generalized Linear Mixed Model (GLMM).

### Descriptives

```{r Increasing-descriptives}
# Copy the data
df_Increasing <- df_moderation

# Ensure Increasing_percentage is numeric
df_Increasing$Increasing_percentage <- as.numeric(df_Increasing$Increasing_percentage)


# Calculate Increasing_n
df_Increasing$Increasing_n <- round((df_Increasing$Increasing_percentage / 100) * df_Increasing$Sample_Size)

# Total number of individuals classified as Increasing
total_Increasing_n <- sum(df_Increasing$Increasing_n, na.rm = TRUE)

# Number of unique samples with Increasing data
unique_Increasing_samples <- length(unique(df_Increasing$Study[!is.na(df_Increasing$Increasing_percentage)]))

# Display as a table
Increasing_info <- data.frame(
  Description = c("Total number of individuals in Increasing trajectory", "Number of unique samples with Increasing data"),
  Value = format(c(total_Increasing_n, unique_Increasing_samples), big.mark = ",")
)

knitr::kable(Increasing_info, caption = "Descriptive Statistics for Increasing Trajectory")
```

### Generic GLMM

```{r Increasing-glmm}

# Apply continuity correction for 0% and 100%
extreme_Increasing <- which(
  !is.na(df_Increasing$Increasing_n) &
  !is.na(df_Increasing$Sample_Size) &
  (df_Increasing$Increasing_n == 0 | df_Increasing$Increasing_n == df_Increasing$Sample_Size)
)

df_Increasing$Increasing_n[extreme_Increasing] <- df_Increasing$Increasing_n[extreme_Increasing] + 0.5
df_Increasing$Sample_Size[extreme_Increasing] <- df_Increasing$Sample_Size[extreme_Increasing] + 1

# Enforce xi <= ni - 0.5 to prevent metafor errors
df_Increasing$Increasing_n <- pmin(df_Increasing$Increasing_n, df_Increasing$Sample_Size - 0.5)

# Fit GLMM with rma.glmm
glmm_Increasing <- rma.glmm(
  measure = "PLO",
  xi = df_Increasing$Increasing_n,
  ni = df_Increasing$Sample_Size,
  data = df_Increasing
)

# Extract estimates
Increasing_prev <- plogis(coef(glmm_Increasing))
Increasing_ci_lower <- plogis(glmm_Increasing$ci.lb)
Increasing_ci_upper <- plogis(glmm_Increasing$ci.ub)

# Number of samples included
n_Increasing_samples <- glmm_Increasing$k

# Create summary table
Increasing_summary <- data.frame(
  Trajectory = "Increasing",
  Pooled_Prevalence = round(Increasing_prev, 3),
  CI_Lower = round(Increasing_ci_lower, 3),
  CI_Upper = round(Increasing_ci_upper, 3),
  samples_included = n_Increasing_samples
)

knitr::kable(Increasing_summary, caption = "Pooled Prevalence Estimate for Increasing Trajectory (GLMM)")

```

### Sensitivity Analysis 1

Exclude Samples with ≥90% Decreasing Proportions

```{r Increasing-sens1}
# Compute Increasing proportion
df_Increasing$Increasing_Proportion <- df_Increasing$Increasing_n / df_Increasing$Sample_Size

# Subset: Exclude samples with Increasing ≥90%
df_Increasing_sens1 <- df_Increasing %>%
  filter(Increasing_Proportion < 0.90)

# Fit GLMM
glmm_Increasing_sens1 <- rma.glmm(
  measure = "PLO",
  xi = Increasing_n,
  ni = Sample_Size,
  data = df_Increasing_sens1
)

# Extract results
sens1_prev <- plogis(coef(glmm_Increasing_sens1))
sens1_ci_lower <- plogis(glmm_Increasing_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_Increasing_sens1$ci.ub)

# Create table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude ≥90% Increasing",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  samples_included = glmm_Increasing_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Increasing Trajectory (Excl. ≥90%)")
```

### Sensitivity Analysis 2

Include Only Samples with N \> 999

```{r Increasing-sens2}
# Subset: samples with Sample_Size > 999
df_Increasing_sens2 <- df_Increasing %>%
  filter(Sample_Size > 999)

# Fit GLMM
glmm_Increasing_sens2 <- rma.glmm(
  measure = "PLO",
  xi = Increasing_n,
  ni = Sample_Size,
  data = df_Increasing_sens2
)

# Extract results
sens2_prev <- plogis(coef(glmm_Increasing_sens2))
sens2_ci_lower <- plogis(glmm_Increasing_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_Increasing_sens2$ci.ub)

# Create table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  samples_included = glmm_Increasing_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Increasing Trajectory (Samples >999)")


```

### Sensitivity Analysis 3

Exclude Influential samples

```{r Increasing-influential}
# Fit glmer model
model_glmer_Increasing <- glmer(cbind(round(Increasing_n), Sample_Size - round(Increasing_n)) ~ 1 + (1 | Study),
                               data = df_Increasing, family = binomial)

# Influence analysis
infl_Increasing <- influence(model_glmer_Increasing, group = "Study")
cooks_Increasing <- cooks.distance(infl_Increasing)

# Define threshold
threshold_Increasing <- 4 / length(cooks_Increasing)

# Identify influential samples
influential_idx_Increasing <- which(cooks_Increasing > threshold_Increasing)
influential_samples_Increasing <- df_Increasing$Study[influential_idx_Increasing]

# Display table
influential_table_Increasing <- data.frame(
  Study_ID = influential_samples_Increasing,
  Cooks_Distance = round(cooks_Increasing[influential_idx_Increasing], 3)
)

# Plot Cook's distances
plot(cooks_Increasing, type = "h", lwd = 2,
     main = "Cook's Distance per sample (Increasing)",
     ylab = "Cook's Distance", xlab = "sample Index")
abline(h = threshold_Increasing, col = "red", lty = 2)

knitr::kable(influential_table_Increasing, caption = "Influential samples in Increasing Trajectory (Cook's Distance)")


```

Excluding Influential samples

```{r Increasing-sens-infl}
# Exclude influential samples
df_Increasing_noinf <- df_Increasing %>%
  filter(!Study %in% influential_samples_Increasing)

# Fit GLMM after exclusion
glmm_Increasing_noinf <- rma.glmm(
  measure = "PLO",
  xi = Increasing_n,
  ni = Sample_Size,
  data = df_Increasing_noinf
)

# Extract results
prev_noinf <- plogis(coef(glmm_Increasing_noinf))
ci_lower_noinf <- plogis(glmm_Increasing_noinf$ci.lb)
ci_upper_noinf <- plogis(glmm_Increasing_noinf$ci.ub)

# Create table
sens_infl_summary_Increasing <- data.frame(
  Analysis = "Excluding Influential samples",
  Pooled_Prevalence = round(prev_noinf, 3),
  CI_Lower = round(ci_lower_noinf, 3),
  CI_Upper = round(ci_upper_noinf, 3),
  samples_included = glmm_Increasing_noinf$k
)

knitr::kable(sens_infl_summary_Increasing, caption = "Sensitivity Analysis: Increasing Trajectory (Excluding Influential samples)")

```

### Use `glmer` estimate

Use with the glmer\`-based intercept estimate:

```{r Increasing-glmer-summary}

# Extract fixed effect estimate
est_glmer_Increasing <- fixef(model_glmer_Increasing)["(Intercept)"]
se_glmer_Increasing <- sqrt(vcov(model_glmer_Increasing)["(Intercept)", "(Intercept)"])

ci_lower_glmer_Increasing <- est_glmer_Increasing - 1.96 * se_glmer_Increasing
ci_upper_glmer_Increasing <- est_glmer_Increasing + 1.96 * se_glmer_Increasing

# Number of samples included in glmer model
n_glmer_samples_Increasing <- length(ranef(model_glmer_Increasing)$Study[[1]])

# Back-transform to probability scale
glmer_summary_Increasing <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_Increasing), 3),
  CI_Lower = round(plogis(ci_lower_glmer_Increasing), 3),
  CI_Upper = round(plogis(ci_upper_glmer_Increasing), 3),
  samples_included = n_glmer_samples_Increasing
)

knitr::kable(glmer_summary_Increasing, caption = "Increasing Trajectory Estimate from glmer Model")
```

## 🩸 High Symptoms Trajectory

We estimate the pooled prevalence of the High trajectory using a Generalized Linear Mixed Model (GLMM).

### Descriptives

```{r High-descriptives}

# Copy the data
df_High <- df_moderation

# Ensure High_percentage is numeric
df_High$High_percentage <- as.numeric(df_High$High_percentage)

# Calculate High_n
df_High$High_n <- round((df_High$High_percentage / 100) * df_High$Sample_Size)


# Total number of individuals classified as High
total_High_n <- sum(df_High$High_n, na.rm = TRUE)

# Number of unique samples with High data
unique_High_samples <- length(unique(df_High$Study[!is.na(df_High$High_percentage)]))

# Display as a table
High_info <- data.frame(
  Description = c("Total number of individuals in High trajectory", "Number of unique samples with High data"),
  Value = format(c(total_High_n, unique_High_samples), big.mark = ",")
)

knitr::kable(High_info, caption = "Descriptive Statistics for High Trajectory")

```

### Generic GLMM

```{r High-glmm}


# Apply continuity correction for 0% and 100%
extreme_High <- which(
  !is.na(df_High$High_n) &
  !is.na(df_High$Sample_Size) &
  (df_High$High_n == 0 | df_High$High_n == df_High$Sample_Size)
)

df_High$High_n[extreme_High] <- df_High$High_n[extreme_High] + 0.5
df_High$Sample_Size[extreme_High] <- df_High$Sample_Size[extreme_High] + 1

# Enforce xi ≤ ni - 0.5 to prevent metafor errors
df_High$High_n <- pmin(df_High$High_n, df_High$Sample_Size - 0.5)

# Fit GLMM with rma.glmm
glmm_High <- rma.glmm(
  measure = "PLO",
  xi = df_High$High_n,
  ni = df_High$Sample_Size,
  data = df_High
)

# Extract estimates
High_prev <- plogis(coef(glmm_High))
High_ci_lower <- plogis(glmm_High$ci.lb)
High_ci_upper <- plogis(glmm_High$ci.ub)

# Number of samples included
n_High_samples <- glmm_High$k

# Create summary table
High_summary <- data.frame(
  Trajectory = "High",
  Pooled_Prevalence = round(High_prev, 3),
  CI_Lower = round(High_ci_lower, 3),
  CI_Upper = round(High_ci_upper, 3),
  samples_included = n_High_samples
)

knitr::kable(High_summary, caption = "Pooled Prevalence Estimate for High Trajectory (GLMM)")

```

### Sensitivity Analysis 1

Exclude Samples with ≥90% Decreasing Proportions

```{r High-sens1}
# Compute High proportion
df_High$High_Proportion <- df_High$High_n / df_High$Sample_Size

# Subset: Exclude samples with High ≥90%
df_High_sens1 <- df_High %>%
  filter(High_Proportion < 0.90)

# Fit GLMM
glmm_High_sens1 <- rma.glmm(
  measure = "PLO",
  xi = High_n,
  ni = Sample_Size,
  data = df_High_sens1
)

# Extract results
sens1_prev <- plogis(coef(glmm_High_sens1))
sens1_ci_lower <- plogis(glmm_High_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_High_sens1$ci.ub)

# Create table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude ≥90% High",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  samples_included = glmm_High_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: High Trajectory (Excl. ≥90%)")
```

### Sensitivity Analysis 2

Include Only Samples with N \> 999

```{r High-sens2}
# Subset: Samples with Sample_Size > 999
df_High_sens2 <- df_High %>%
  filter(Sample_Size > 999)

# Fit GLMM
glmm_High_sens2 <- rma.glmm(
  measure = "PLO",
  xi = High_n,
  ni = Sample_Size,
  data = df_High_sens2
)

# Extract results
sens2_prev <- plogis(coef(glmm_High_sens2))
sens2_ci_lower <- plogis(glmm_High_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_High_sens2$ci.ub)

# Create table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  samples_included = glmm_High_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: High Trajectory (Samples >999)")

```

### Sensitivity Analysis 3

Exclude Influential samples

```{r High-influential}
# Fit glmer model
model_glmer_High <- glmer(cbind(round(High_n), Sample_Size - round(High_n)) ~ 1 + (1 | Study),
                              data = df_High, family = binomial)

# Influence analysis
infl_High <- influence(model_glmer_High, group = "Study")
cooks_High <- cooks.distance(infl_High)

# Define threshold
threshold_High <- 4 / length(cooks_High)

# Plot Cook's distances
plot(cooks_High, type = "h", lwd = 2,
     main = "Cook's Distance per sample (High)",
     ylab = "Cook's Distance", xlab = "sample Index")
abline(h = threshold_High, col = "red", lty = 2)

# Identify influential samples
influential_idx_High <- which(cooks_High > threshold_High)
influential_samples_High <- df_High$Study[influential_idx_High]

# Display table
influential_table_High <- data.frame(
  Study_ID = influential_samples_High,
  Cooks_Distance = round(cooks_High[influential_idx_High], 3)
)

knitr::kable(influential_table_High, caption = "Influential samples in High Trajectory (Cook's Distance)")

```

Exclude Influential samples

```{r High-sens-infl}
# Exclude influential samples
df_High_noinf <- df_High %>%
  filter(!Study %in% influential_samples_High)

# Fit GLMM after exclusion
glmm_High_noinf <- rma.glmm(
  measure = "PLO",
  xi = High_n,
  ni = Sample_Size,
  data = df_High_noinf
)

# Extract results
prev_noinf <- plogis(coef(glmm_High_noinf))
ci_lower_noinf <- plogis(glmm_High_noinf$ci.lb)
ci_upper_noinf <- plogis(glmm_High_noinf$ci.ub)

# Create table
sens_infl_summary_High <- data.frame(
  Analysis = "Excluding Influential samples",
  Pooled_Prevalence = round(prev_noinf, 3),
  CI_Lower = round(ci_lower_noinf, 3),
  CI_Upper = round(ci_upper_noinf, 3),
  samples_included = glmm_High_noinf$k
)

knitr::kable(sens_infl_summary_High, caption = "Sensitivity Analysis: High Trajectory (Excluding Influential samples)")

```

### Use `glmer`estimate

Use with the glmer\`-based intercept estimate:

```{r High-glmer-summary}

# Extract fixed effect estimate
est_glmer_High <- fixef(model_glmer_High)["(Intercept)"]
se_glmer_High <- sqrt(vcov(model_glmer_High)["(Intercept)", "(Intercept)"])

ci_lower_glmer_High <- est_glmer_High - 1.96 * se_glmer_High
ci_upper_glmer_High <- est_glmer_High + 1.96 * se_glmer_High

# Number of samples included in glmer model
n_glmer_samples_High <- length(ranef(model_glmer_High)$Study[[1]])

# Back-transform to probability scale
glmer_summary_High <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_High), 3),
  CI_Lower = round(plogis(ci_lower_glmer_High), 3),
  CI_Upper = round(plogis(ci_upper_glmer_High), 3),
  samples_included = n_glmer_samples_High
)

knitr::kable(glmer_summary_High, caption = "High Trajectory Estimate from glmer Model")

```

## 🌗 Moderate Symptoms Trajectory

We estimate the pooled prevalence of the Moderate trajectory using a Generalized Linear Mixed Model (GLMM)

### Descriptives

```{r moderate-descriptives}

# Copy the data
df_moderate <- df_moderation


# Ensure Moderate_percentage is numeric
df_moderate$Moderate_percentage <- as.numeric(df_moderate$Moderate_percentage)

# Calculate Moderate_n
df_moderate$Moderate_n <- round((df_moderate$Moderate_percentage / 100) * df_moderate$Sample_Size)

total_Moderate_n <- sum(df_moderate$Moderate_n, na.rm = TRUE)

# Number of unique samples with Moderate data

unique_Moderate_samples <- length(unique(df_moderate$Study[!is.na(df_moderate$Moderate_percentage)]))

# Total number of individuals classified as Moderate


Moderate_info <- data.frame(
  Description = c("Total number of individuals in Moderate trajectory", "Number of unique samples with Moderate data"),
  Value = format(c(total_Moderate_n, unique_Moderate_samples), big.mark = ",")
)

knitr::kable(Moderate_info, caption = "Descriptive Statistics for Moderate Trajectory")
```

### Generic GLMM

```{r moderate-glmm}



# Apply continuity correction for 0% and 100%
extreme_Moderate <- which(!is.na(df_moderate$Moderate_n) &
                           !is.na(df_moderate$Sample_Size) &
                           (df_moderate$Moderate_n == 0 | df_moderate$Moderate_n == df_moderate$Sample_Size))

df_moderate$Moderate_n[extreme_Moderate] <- df_moderate$Moderate_n[extreme_Moderate] + 0.5
df_moderate$Sample_Size[extreme_Moderate] <- df_moderate$Sample_Size[extreme_Moderate] + 1

# Prevent xi > ni by enforcing xi <= ni - 0.5
df_moderate$Moderate_n <- pmin(df_moderate$Moderate_n, df_moderate$Sample_Size - 0.5)


# Fit GLMM
glmm_Moderate <- rma.glmm(measure = "PLO",
                           xi = df_moderate$Moderate_n,
                           ni = df_moderate$Sample_Size,
                           data = df_moderate)

# Extract estimates
Moderate_prev <- plogis(coef(glmm_Moderate))
Moderate_ci_lower <- plogis(glmm_Moderate$ci.lb)
Moderate_ci_upper <- plogis(glmm_Moderate$ci.ub)

# Create summary table
Moderate_summary <- data.frame(
  Trajectory = "Moderate",
  Pooled_Prevalence = round(Moderate_prev, 3),
  CI_Lower = round(Moderate_ci_lower, 3),
  CI_Upper = round(Moderate_ci_upper, 3),
  samples_included = glmm_Moderate$k
)

knitr::kable(Moderate_summary, caption = "Pooled Prevalence Estimate for Moderate  Trajectory (GLMM)")
```

### Sensitivity Analysis 1

Exclude Samples with ≥90% Prevalence

```{r moderate-sens1}
df_moderate$Moderate_Proportion <- df_moderate$Moderate_n / df_moderate$Sample_Size

df_moderate_sens1 <- df_moderate %>%
  filter(Moderate_Proportion < 0.90)

glmm_Moderate_sens1 <- rma.glmm(measure = "PLO",
                                 xi = Moderate_n,
                                 ni = Sample_Size,
                                 data = df_moderate_sens1)

sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude ≥90% Moderate ",
  Pooled_Prevalence = round(plogis(coef(glmm_Moderate_sens1)), 3),
  CI_Lower = round(plogis(glmm_Moderate_sens1$ci.lb), 3),
  CI_Upper = round(plogis(glmm_Moderate_sens1$ci.ub), 3),
  samples_included = glmm_Moderate_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Moderate  Trajectory (Excl. ≥90%)")
```

### Sensitivity Analysis 2

Only Large Samples (N \> 999)

```{r moderate-sens2}
df_moderate_sens2 <- df_moderate %>%
  filter(Sample_Size > 999)

glmm_Moderate_sens2 <- rma.glmm(measure = "PLO",
                                 xi = Moderate_n,
                                 ni = Sample_Size,
                                 data = df_moderate_sens2)

sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(plogis(coef(glmm_Moderate_sens2)), 3),
  CI_Lower = round(plogis(glmm_Moderate_sens2$ci.lb), 3),
  CI_Upper = round(plogis(glmm_Moderate_sens2$ci.ub), 3),
  samples_included = glmm_Moderate_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Moderate  Trajectory (Samples >999)")
```

### Sensitivity Analysis 3

Exclude Influential samples

```{r moderate-influential}
model_glmer_Moderate <- glmer(cbind(round(Moderate_n), Sample_Size - round(Moderate_n)) ~ 1 + (1 | Study),
                               data = df_moderate, family = binomial)

infl_Moderate <- influence(model_glmer_Moderate, group = "Study")
cooks_Moderate <- cooks.distance(infl_Moderate)

threshold_Moderate <- 4 / length(cooks_Moderate)

plot(cooks_Moderate, type = "h", lwd = 2,
     main = "Cook's Distance per Study (Moderate)",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold_Moderate, col = "red", lty = 2)

influential_idx <- which(cooks_Moderate > threshold_Moderate)
influential_samples <- df_moderate$Study[influential_idx]

influential_table <- data.frame(
  Study_ID = influential_samples,
  Cooks_Distance = round(cooks_Moderate[influential_idx], 3)
)

knitr::kable(influential_table, caption = "Influential samples in Moderate Trajectory (Cook's Distance)")
```

Refit Model Excluding Influential samples

```{r moderate-sens-infl}
df_Moderate_noinf <- df_moderate %>% filter(!Study %in% influential_studies)

glmm_Moderate_noinf <- rma.glmm(measure = "PLO",
                       xi = Moderate_n,
                       ni = Sample_Size,
                       data = df_Moderate_noinf)

sens_infl_summary <- data.frame(
  Analysis = "Excluding Influential Studies",
  Pooled_Prevalence = round(plogis(coef(glmm_Moderate_noinf)), 3),
  CI_Lower = round(plogis(glmm_Moderate_noinf$ci.lb), 3),
  CI_Upper = round(plogis(glmm_Moderate_noinf$ci.ub), 3),
  Studies_included = glmm_Moderate_noinf$k
)

knitr::kable(sens_infl_summary, caption = "Sensitivity Analysis: Moderate Trajectory (Excluding Influential Studies)")
```

### Use `glmer`estimate

```{r moderate-glmer-summary}
est_glmer <- fixef(model_glmer_Moderate)["(Intercept)"]
se_glmer <- sqrt(vcov(model_glmer_Moderate)["(Intercept)", "(Intercept)"])

ci_lower <- est_glmer - 1.96 * se_glmer
ci_upper <- est_glmer + 1.96 * se_glmer

n_glmer_samples <- length(ranef(model_glmer_Moderate)$Study[[1]])

final_summary <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer), 3),
  CI_Lower = round(plogis(ci_lower), 3),
  CI_Upper = round(plogis(ci_upper), 3),
  samples_included = n_glmer_samples
)

knitr::kable(final_summary, caption = "Moderate  Trajectory Estimate from glmer Model")
```
# Tables for Paper
## Prevalences Summary Table (Table 1)
```{r prevalences-summary-table}
# Trajectory order & fitted models
traj_order <- c("Low","Decreasing","Increasing","High","Moderate")

fits <- list(
  Low        = glmm_Low,
  Decreasing = glmm_Decreasing,
  Increasing = glmm_Increasing,
  High       = glmm_High,
  Moderate   = glmm_Moderate
)

trajectory_counts <- tibble(
  trajectory = factor(traj_order, levels = traj_order),
  N = c(
    total_Low_n,
    total_Decreasing_n,
    total_Increasing_n,
    total_High_n,
    total_Moderate_n
  )
) |>
  mutate(pct = round(100 * N / sum(N), 1))

k_vec <- c(
  Low        = as.integer(unique_Low_samples),
  Decreasing = as.integer(unique_Decreasing_samples),
  Increasing = as.integer(unique_Increasing_samples),
  High       = as.integer(unique_High_samples),
  Moderate   = as.integer(unique_Moderate_samples)
)

# build one row per trajectory from the GLMM fit
row_from <- function(name, fit) {
  pr <- predict(fit, transf = transf.ilogit)
  tibble(
    trajectory = name,
    rel_prev   = sprintf("%.1f%%", pr$pred * 100),
    ci_95      = sprintf("%.1f to %.1f%%", pr$ci.lb * 100, pr$ci.ub * 100),
    k          = k_vec[[name]],
    N_pct      = sprintf(
                   "%s (%.1f%%)",
                   format(trajectory_counts$N[trajectory_counts$trajectory == name], big.mark = ","),
                   trajectory_counts$pct[trajectory_counts$trajectory == name]
                 ),
    logit      = round(unname(coef(fit)[1]), 2),
    tau2       = round(as.numeric(fit$tau2), 2)
  )
}

table1_df <- bind_rows(lapply(traj_order, function(nm) row_from(nm, fits[[nm]])))

# Render table
table1_df |>
  gt() |>
  cols_label(
    trajectory = "PTSD Symptom Trajectory",
    rel_prev   = "Relative Prevalence",
    ci_95      = "95%(CI)",
    k          = "k",
    N_pct      = "N (%)",
    logit      = "Logit Estimate",
    tau2       = html("&tau;<sup>2</sup>")
  ) |>
  tab_header(
    title    = md("**Table 1**"),
    subtitle = md("Relative Prevalence of PTSD Symptom Trajectories")
  ) |>
  cols_align("left",   columns = trajectory) |>
  cols_align("center", columns = c(rel_prev, ci_95, k, N_pct, logit, tau2)) |>
  fmt_integer(columns = k, use_seps = TRUE) |>
  fmt_number(columns = c(logit, tau2), decimals = 2) |>
  opt_row_striping() |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) |>
  tab_options(
    table.width = pct(100),
    data_row.padding = px(6),
    table.font.size = px(13)
  ) |>
  tab_source_note(
    source_note = html("<em>Note.</em> k represents the number of unique samples. Logit estimate reflects pooled prevalence on a log-odds scale. &tau;<sup>2</sup> indicates between-sample heterogeneity; values &gt; 0.50 suggest substantial heterogeneity.")
  )
```

## Sensitivity Analyses Summary

```{r sensitivity-summary-table}
# ---- helper: one rma.glmm -> 1 row (no changes) ----
mk_row <- function(fit, label) {
  prev  <- plogis(as.numeric(coef(fit)))
  ci_lb <- plogis(as.numeric(fit$ci.lb))
  ci_ub <- plogis(as.numeric(fit$ci.ub))
  tibble(
    Analysis = label,
    k        = as.integer(fit$k),
    `Relative Prevalence (95% CI)` =
      sprintf("%.1f%% [%.1f%% – %.1f%%]", 100 * prev, 100 * ci_lb, 100 * ci_ub),
    `Logit Estimate` = as.numeric(coef(fit)),
    tau2             = if (!is.null(fit$tau2)) as.numeric(fit$tau2) else NA_real_
  )
}

# ---- 1. DEFINE components ----
trajectory_map <- c(
  "Low"        = "Low symptoms",
  "Decreasing" = "Decreasing symptoms",
  "Increasing" = "Increasing symptoms",
  "High"       = "High symptoms",
  "Moderate"   = "Moderate symptoms"
)

analysis_map <- c(
  "Main"  = "Main analysis (all studies)",
  "sens1" = "Excluding studies with extreme prevalence (≥90%)",
  "sens2" = "Including only large-sample studies (N ≥ 1000)",
  "noinf" = "Excluding influential studies"
)

# ---- 2. BUILD the table of specifications programmatically ----
all_specs <- crossing(
  trajectory_stub = names(trajectory_map),
  analysis_stub   = names(analysis_map)
) |>
  mutate(
    object_name = if_else(
      analysis_stub == "Main",
      paste0("glmm_", trajectory_stub),
      paste0("glmm_", trajectory_stub, "_", analysis_stub)
    ),
    Trajectory = trajectory_map[trajectory_stub],
    Analysis   = analysis_map[analysis_stub]
  )

# ---- 3. EXECUTE: Iterate over specs to build the final table ----
big_tbl <- pmap_dfr(
  all_specs,
  function(object_name, Trajectory, Analysis, ...) {
    fit <- tryCatch(get(object_name, envir = .GlobalEnv), error = \(e) NULL)
    if (!is.null(fit)) {
      mk_row(fit, Analysis) |>
        mutate(Trajectory = Trajectory, .before = 1)
    }
  }
)

# ---- 4. PREPARE for gt: Enforce group order ----
trajectory_order <- c(
  "Low symptoms",
  "Decreasing symptoms",
  "Increasing symptoms",
  "High symptoms",
  "Moderate symptoms"
)

big_tbl_ordered <- big_tbl |>
  mutate(Trajectory = factor(Trajectory, levels = trajectory_order)) |>
  arrange(Trajectory)

# ---- 5. CREATE the gt table with final formatting ----
gt_all <-
  gt(big_tbl_ordered, groupname_col = "Trajectory") |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_row_groups()
  ) |>
  cols_label(
    Analysis = "Analysis",
    k = html("k"),
    `Relative Prevalence (95% CI)` = "Relative Prevalence (95% CI)",
    `Logit Estimate` = "Logit Estimate",
    tau2 = html("&tau;<sup>2</sup>")
  ) |>
  fmt_number(columns = c(`Logit Estimate`, tau2), decimals = 2) |>
  cols_align(align = "left", columns = Analysis) |>
  cols_align(align = "center", columns = c(k, `Relative Prevalence (95% CI)`, `Logit Estimate`, tau2)) |>
  opt_row_striping() |>
  tab_options(
    table.font.names = c("Source Sans Pro", "Inter", "Helvetica", "Arial", "sans-serif"),
    data_row.padding = px(6),
    column_labels.font.weight = "bold",
    row_group.as_column = FALSE
  ) |>
  tab_source_note(md(
    "Note. Logit estimate is on the log-odds scale; &tau;<sup>2</sup> indicates between-study heterogeneity; values > 0.50 suggest substantial heterogeneity."
  ))

# ---- 6) Auto-notes using Study (already "Author, Year") ----

# Minimal helpers
.get_model_data <- function(fit) {
  d <- tryCatch({
    cd <- fit$call$data
    if (is.symbol(cd)) get(as.character(cd), envir = .GlobalEnv)
    else if (is.character(cd)) get(cd, envir = .GlobalEnv)
    else NULL
  }, error = function(e) NULL)
  if (is.null(d)) tryCatch(model.frame(fit), error = function(e) NULL) else d
}

.used_rows <- function(fit, n_df) {
  if (!is.null(fit$not.na) && length(fit$not.na) == n_df) which(fit$not.na) else seq_len(n_df)
}

.labels_from_study <- function(fit) {
  df <- .get_model_data(fit)
  if (is.null(df)) return(character(0))
  nm <- if ("Study" %in% names(df)) "Study" else if ("study" %in% names(df)) "study" else stop("Column 'Study' not found.")
  idx <- .used_rows(fit, nrow(df))
  as.character(df[[nm]])[idx]
}

.excluded_study_labels <- function(traj_stub) {
  main_fit  <- tryCatch(get(paste0("glmm_", traj_stub), envir = .GlobalEnv), error = function(e) NULL)
  noinf_fit <- tryCatch(get(paste0("glmm_", traj_stub, "_noinf"), envir = .GlobalEnv), error = function(e) NULL)
  if (is.null(main_fit)) return(character(0))
  main_labs <- trimws(.labels_from_study(main_fit))
  if (!is.null(noinf_fit)) {
    noinf_labs <- trimws(.labels_from_study(noinf_fit))
    setdiff(main_labs, noinf_labs)
  } else {
    character(0)  # no _noinf model => no exclusions to show
  }
}

format_note <- function(vec) {
  if (length(vec) == 0) "No influential studies excluded."
  else paste0("Excluded influential studies: ", paste(vec, collapse = "; "))
}

# Build notes per displayed group label
excluded_by_group <- setNames(
  lapply(names(trajectory_map), .excluded_study_labels),
  nm = unname(trajectory_map[names(trajectory_map)])
)
excluded_notes <- lapply(excluded_by_group, format_note)

# Attach footnote to the “Excluding influential studies” row within each group
gt_all <- purrr::reduce(
  names(excluded_notes),
  .init = gt_all,
  .f = function(gt_obj, grp_label) {
    tab_footnote(
      gt_obj,
      footnote  = excluded_notes[[grp_label]],
      locations = cells_body(
        columns = "Analysis",
        rows = (Trajectory == grp_label) & (Analysis == "Excluding influential studies")
      )
    )
  }
)

gt_all
```
------------------------------------------------------------------------




