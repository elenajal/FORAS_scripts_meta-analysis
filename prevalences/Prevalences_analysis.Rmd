---
title: "Generalized Linear Mixed Models for Prevalences"
date: "2025-07-20"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    toc_depth: 4
    number_sections: true
    code_folding: hide
  word_document:
    toc: true
    toc_depth: '4'
  pdf_document:
    toc: true
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = FALSE, cache = FALSE)

# Set the root directory for the entire document (recommended way)
knitr::opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))

```

# Preparation

```{r load-packages}
library(readr)
library(metafor)
library(lme4)
library(influence.ME)
library(dplyr)
library(gt)
library(purrr)
library(tidyr)
```

## Load Data

```{r load-data}
# load data
df_moderation <- read.csv2("../pre-processing/output/data_for_moderation_analyses.csv", dec = ".")

# Display the first rows to check the data
head(df_moderation)
```

## Descriptive Statistics

We provide basic descriptive statistics of the included studies, without excluding any data due to missingness.

```{r descriptives}
# Calculate descriptives
total_number_of_samples <- nrow(df_moderation)
total_sample_size <- sum(df_moderation$Sample_Size, na.rm = TRUE)

# Create a summary table
summary_table <- data.frame(
  Description = c("Number of studies / cohorts", "Total sample size (sum of available Sample_Size)"),
  Value = format(c(total_number_of_samples, total_sample_size), big.mark = ",")
)

# Display as a table
knitr::kable(summary_table, caption = "Descriptive Statistics of the Dataset")
```

### Descriptives Tables for All Studies

```{r descriptives-studies-table}
overview <- tibble(
  `Authors (Study)` = df_moderation$Study,
  `Sample Size`     = df_moderation$Sample_Size,
  `Mean Age`        = df_moderation$Mean_age,
  `Percentage Women`= df_moderation$Percentage_women,
  `Assessed Trauma Type` = df_moderation$Assessed_trauma_type,
  `Number of TP assessments`  = df_moderation$TP_assessments,
  `Number of Trajectories Found` = df_moderation$N_trajectories

  ) |> 
  arrange(`Authors (Study)`)

if (knitr::is_html_output() || knitr::is_latex_output()) {
  fmt <- if (knitr::is_latex_output()) "latex" else "html"

  overview %>%
    mutate(
      `Sample Size` = format(`Sample Size`, big.mark = ",", trim = TRUE),
      `Mean Age` = ifelse(is.na(`Mean Age`), NA, sprintf("%.1f", `Mean Age`)),
      `Percentage Women` = ifelse(is.na(`Percentage Women`), NA, sprintf("%.1f%%", `Percentage Women`))
    ) |>
    knitr::kable(format = fmt, caption = "Study Overview", booktabs = TRUE, linesep = "") |>
    kableExtra::kable_styling(full_width = FALSE, position = "left",
                              bootstrap_options = c("striped", "hover", "condensed")) |>
    kableExtra::column_spec(1, bold = TRUE) |>
    kableExtra::scroll_box(height = "470px")
}
```
# Prevalences

```{r portable-glmm-function}
# Portable function for pooled prevalence via GLMM with continuity correction
fit_glmm_prev <- function(df, event_col, n_col = "Sample_Size", label = event_col) {
  xi <- suppressWarnings(as.numeric(df[[event_col]]))
  ni <- suppressWarnings(as.numeric(df[[n_col]]))

  extreme <- !is.na(xi) & !is.na(ni) & (xi == 0 | xi == ni)
  xi[extreme] <- xi[extreme] + 0.5
  ni[extreme] <- ni[extreme] + 1

  fit <- metafor::rma.glmm(measure = "PLO", xi = xi, ni = ni, data = df)
  pr  <- predict(fit, transf = metafor::transf.ilogit)

  data.frame(
    Analysis        = label,
    Pooled_Prevalence = round(as.numeric(pr$pred), 3),
    CI_Lower          = round(as.numeric(pr$ci.lb), 3),
    CI_Upper          = round(as.numeric(pr$ci.ub), 3),
    Samples_included  = fit$k,
    Logit_Estimate    = round(unname(coef(fit)[1]), 3),
    Ï„2                = round(as.numeric(fit$tau2), 3),
    check.names = FALSE
  )
}
```

## ðŸš€ Low Symptom Trajectory

### Descriptives

We provide additional descriptives based on the Low trajectory variable, using the full dataset.

```{r Low-descriptives}

# Copy the dataset to avoid overwriting the original
df_low <- df_moderation

# Ensure Low_percentage is numeric
df_low$Low_percentage <- as.numeric(df_low$Low_percentage)


# Calculate Low_n as (percentage / 100) * sample size
df_low$Low_n <- round((df_low$Low_percentage / 100) * df_low$Sample_Size)

# Total number of individuals classified as Low (ignoring missingness in other variables)
total_Low_n <- sum(df_low$Low_n, na.rm = TRUE)

# Number of unique samples (assumes a column Study exists)
unique_Low_studies <- length(unique(df_low$Study[!is.na(df_low$Low_percentage)]))

# Display as a table
additional_info <- data.frame(
  Description = c("Total number of individuals in Low trajectory", "Number of unique studies"),
  Value = format(c(total_Low_n, unique_Low_studies), big.mark = ",")
)

knitr::kable(additional_info, caption = "Additional Descriptives for Low Trajectory Analysis")
```

### Generic GLMM results

We estimate the pooled prevalence of the **Low trajectory** using a Generalized Linear Mixed Model (GLMM).\
We apply a continuity correction of 0.5 individuals for studies reporting exactly 0% or 100% Low cases.\
No data is removed from the dataset; the `rma.glmm()` function handles missingness internally.

```{r Low-glmm}
# Fit GLMM
glmm_Low <- fit_glmm_prev(df_low, event_col = "Low_n", n_col = "Sample_Size",
                             label = "Main Analysis")

# Display result
knitr::kable(glmm_Low, caption = "Pooled Prevalence Estimate for Low Trajectory (GLMM)")
```

### Sensitivity Analysis 1

Excluding samples with Low trajectory â‰¥90% of the sample

```{r Low-sensitivity_1}
# Subset: Exclude samples with Low trajectory â‰¥90% of the sample
df_low_sens1 <- subset(df_low,
                   !is.na(Low_n) & !is.na(Sample_Size) &
                   !is.na(Low_percentage) & Low_percentage < 90)

# Fit GLMM
glmm_Low_sens1 <-  fit_glmm_prev(df_low_sens1, "Low_n", "Sample_Size", "Sensitivity 1: Exclude â‰¥90% Low")

# Display table
knitr::kable(glmm_Low_sens1, caption = "Sensitivity Analysis 1: Low Trajectory (Excl. â‰¥90% Prevalence)")
```

### Sensitivity Analysis 2

Including only large samples (N \> 999)

```{r Low-sensitivity-2}
# Subset: Include only samples with Sample_Size > 999
df_low_sens2 <- subset(df_low,
                   !is.na(Low_n) & !is.na(Sample_Size) &
                   Sample_Size > 999)

# Fit GLMM
glmm_Low_sens2 <- fit_glmm_prev(df_low_sens2, "Low_n", "Sample_Size", "Sensitivity 2: N > 999")

# Display table
knitr::kable(glmm_Low_sens2, caption = "Sensitivity Analysis 2: Low Trajectory (Samples > 999)")
```

### Sensitivity Analysis 3

Exclude Influential Studies: We conducted an influence analysis to identify studies with disproportionate impact on the pooled estimate using Cook's distance from a glmer model.

```{r Low-influential}
# Fit a GLMM with glmer (binomial model with random study effect)
model_glmer_Low <- glmer(cbind(round(Low_n), Sample_Size - round(Low_n)) ~ 1 + (1 | Study),
                     data = df_low,
                     family = binomial)

# Run influence analysis by Studies
infl <- influence(model_glmer_Low, group = "Study")

# Compute Cook's distance
cooks <- cooks.distance(infl)

# Define rule-of-thumb threshold
threshold <- 4 / length(cooks)

# Plot Cook's distances
p1 <- plot(cooks, type = "h", lwd = 2,
     main = "Cook's Distance per Study",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold, col = "red", lty = 2)

# Identify studies above the threshold
influential_indices <- which(cooks > threshold)
influential_studies_Low <- unique(df_low$Study[influential_indices])

# Display studies
influential_table <- data.frame(
  Study_ID = influential_studies_Low,
  Cooks_Distance = round(cooks[influential_indices], 3)
)

knitr::kable(influential_table, caption = "Studies Identified as Influential (Cook's Distance)")
```

We reran the GLMM excluding the identified influential studies.

```{r Low-sens-influential}
# Exclude the influential studies
df_Low_noinf <- df_low %>%
  filter(!Study %in% influential_studies_Low)

# Refit GLMM (metafor version)
glmm_Low_noinf <- fit_glmm_prev(df_Low_noinf, "Low_n", "Sample_Size",
                           "Sensitivity 3: Excluding Influential Studies")

# Display table
knitr::kable(glmm_Low_noinf, caption = "Sensitivity Analysis 3: Low Trajectory (Excluding Influential Studies)")
```

### Use `glmer` estimate

with the glmer\`-based intercept estimate:

```{r Low-glmer-summary}
# Extract fixed effect estimate
est_glmer_Low <- fixef(model_glmer_Low)["(Intercept)"]
se_glmer_Low <- sqrt(vcov(model_glmer_Low)["(Intercept)", "(Intercept)"])

# Compute 95% CI (logit scale)
ci_lower_glmer_Low <- est_glmer_Low - 1.96 * se_glmer_Low
ci_upper_glmer_Low <- est_glmer_Low + 1.96 * se_glmer_Low

n_glmer_studies_Low <- length(ranef(model_glmer_Low)$Study[[1]])

# Run glmer
glmer_summary_Low <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_Low), 3),
  CI_Lower = round(plogis(ci_lower_glmer_Low), 3),
  CI_Upper = round(plogis(ci_upper_glmer_Low), 3), 
  Studies_included = n_glmer_studies_Low
)

knitr::kable(glmer_summary_Low, caption = "Low Trajectory Estimate from glmer Model")
```

**Note on Number of Studies Included** The number of studies included in the models may differ between rma.glmm() and glmer(). This is expected because the two methods handle data slightly differently:

rma.glmm() is designed for meta-analysis and accepts aggregated data (events + sample size). It applies continuity corrections directly and may include more studies when appropriate.

glmer() is a general GLMM function. It uses binomial counts and handles missingness or zero counts differently. Studies with no variance or missing outcomes may be excluded automatically.

Both approaches are valid, but they may produce slightly different sample sizes. We report both transparently for completeness.

## ðŸŒ¿ Decreasing Symptoms Trajectory

We estimate the pooled prevalence of the Decreasing trajectory using a Generalized Linear Mixed Model (GLMM).

### Descriptives

```{r Decreasing-descriptives}

# Copy the data
df_Decreasing <- df_moderation

# Ensure Decreasing_percentage is numeric
df_Decreasing$Decreasing_percentage <- as.numeric(df_Decreasing$Decreasing_percentage)


# Calculate Decreasing_n
df_Decreasing$Decreasing_n <- round((df_Decreasing$Decreasing_percentage / 100) * df_Decreasing$Sample_Size)

# Total number of individuals classified as Decreasing
total_Decreasing_n <- sum(df_Decreasing$Decreasing_n, na.rm = TRUE)

# Number of unique samples with Decreasing data
unique_Decreasing_studies <- length(unique(df_Decreasing$Study[!is.na(df_Decreasing$Decreasing_percentage)]))

# Display as a table
Decreasing_info <- data.frame(
  Description = c("Total number of individuals in Decreasing trajectory", "Number of unique studies with Decreasing data"),
  Value = format(c(total_Decreasing_n, unique_Decreasing_studies), big.mark = ",")
)

knitr::kable(Decreasing_info, caption = "Descriptive Statistics for Decreasing Trajectory")

```

### Generic GLMM

```{r Decreasing-glmm}
### Generic GLMM

# Compute raw number of recoveries BEFORE rounding
df_Decreasing <- df_moderation %>%
  mutate(
    Decreasing_percentage = as.numeric(Decreasing_percentage),
    Decreasing_prop = Decreasing_percentage / 100,
    Decreasing_n_raw = Decreasing_prop * Sample_Size
  )

# Apply continuity correction BEFORE rounding
extreme_cases <- !is.na(df_Decreasing$Decreasing_n_raw) & 
           (df_Decreasing$Decreasing_n_raw == 0 | df_Decreasing$Decreasing_n_raw == df_Decreasing$Sample_Size)

df_Decreasing$Decreasing_n_raw[extreme_cases] <- df_Decreasing$Decreasing_n_raw[extreme_cases] + 0.5
df_Decreasing$Sample_Size[extreme_cases] <- df_Decreasing$Sample_Size[extreme_cases] + 1

df_Decreasing$Decreasing_n_raw <- pmin(df_Decreasing$Decreasing_n_raw, df_Decreasing$Sample_Size - 0.5)


# Round to integer after correction
df_Decreasing$Decreasing_n <- round(df_Decreasing$Decreasing_n_raw)

# Sanity check: raw prevalence
df_Decreasing$Raw_Prevalence <- df_Decreasing$Decreasing_n / df_Decreasing$Sample_Size
summary(df_Decreasing$Raw_Prevalence)



# Fit GLMM
glmm_Decreasing <- rma.glmm(
  measure = "PLO",
  xi = df_Decreasing$Decreasing_n,
  ni = df_Decreasing$Sample_Size,
  data = df_Decreasing
)

# Back-transform
Decreasing_prev <- plogis(coef(glmm_Decreasing))
Decreasing_ci_lower <- plogis(glmm_Decreasing$ci.lb)
Decreasing_ci_upper <- plogis(glmm_Decreasing$ci.ub)

# Output
Decreasing_summary <- data.frame(
  Trajectory = "Decreasing",
  Pooled_Prevalence = round(Decreasing_prev, 3),
  CI_Lower = round(Decreasing_ci_lower, 3),
  CI_Upper = round(Decreasing_ci_upper, 3),
  Studies_included = glmm_Decreasing$k
)

knitr::kable(Decreasing_summary, caption = "Pooled Prevalence Estimate for Decreasing Trajectory (GLMM)")

```

### Sensitivity Analysis 1

Exclude samples â‰¥90% Decreasing Proportions

```{r Decreasing-sens1}
# Compute Decreasing proportion
df_Decreasing$Decreasing_Proportion <- df_Decreasing$Decreasing_n / df_Decreasing$Sample_Size

# Subset: Exclude samples with Decreasing â‰¥90%
df_Decreasing_sens1 <- df_Decreasing %>%
  filter(Decreasing_Proportion < 0.90)

# Fit GLMM
glmm_Decreasing_sens1 <- rma.glmm(
  measure = "PLO",
  xi = Decreasing_n,
  ni = Sample_Size,
  data = df_Decreasing_sens1
)

# Extract results
sens1_prev <- plogis(coef(glmm_Decreasing_sens1))
sens1_ci_lower <- plogis(glmm_Decreasing_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_Decreasing_sens1$ci.ub)

# Create table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude â‰¥90% Decreasing",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  Studies_included = glmm_Decreasing_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Decreasing Trajectory (Excl. â‰¥90%)")
```

### Sensitivity Analysis 2

Include Only Samples with N \> 999

```{r Decreasing-sens2}
# Subset: Samples with Sample_Size > 999
df_Decreasing_sens2 <- df_Decreasing %>%
  filter(Sample_Size > 999)

# Fit GLMM
glmm_Decreasing_sens2 <- rma.glmm(
  measure = "PLO",
  xi = Decreasing_n,
  ni = Sample_Size,
  data = df_Decreasing_sens2
)

# Extract results
sens2_prev <- plogis(coef(glmm_Decreasing_sens2))
sens2_ci_lower <- plogis(glmm_Decreasing_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_Decreasing_sens2$ci.ub)

# Create table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  Studies_included = glmm_Decreasing_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Decreasing Trajectory (Samples >999)")
```

### Sensitivity Analysis 3

Exclude Influential Studies

```{r Decreasing-influential}
# Fit glmer model
model_glmer_Decreasing <- glmer(cbind(round(Decreasing_n), Sample_Size - round(Decreasing_n)) ~ 1 + (1 | Study),
                              data = df_Decreasing, family = binomial)

# Influence analysis
infl_Decreasing <- influence(model_glmer_Decreasing, group = "Study")
cooks_Decreasing <- cooks.distance(infl_Decreasing)

# Define threshold
threshold_Decreasing <- 4 / length(cooks_Decreasing)

# Plot Cook's distances for Decreasing
plot(cooks_Decreasing, type = "h", lwd = 2,
     main = "Cook's Distance per Study (Decreasing)",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold_Decreasing, col = "red", lty = 2)


# Identify influential studies
influential_idx <- which(cooks_Decreasing > threshold_Decreasing)
influential_studies_Decreasing <- df_Decreasing$Study[influential_idx]

# Display table
influential_table_Decreasing <- data.frame(
  Study_ID = influential_studies_Decreasing,
  Cooks_Distance = round(cooks_Decreasing[influential_idx], 3)
)

knitr::kable(influential_table_Decreasing, caption = "Influential Studies in Decreasing Trajectory (Cook's Distance)")
```

Exclude Influential Studies

```{r Decreasing-sens-infl}
# Exclude influential studies
df_Decreasing_noinf <- df_Decreasing %>%
  filter(!Study %in% influential_studies_Decreasing)

# Fit GLMM after exclusion
glmm_Decreasing_noinf <- rma.glmm(
  measure = "PLO",
  xi = Decreasing_n,
  ni = Sample_Size,
  data = df_Decreasing_noinf
)

# Extract results
prev_noinf <- plogis(coef(glmm_Decreasing_noinf))
ci_lower_noinf <- plogis(glmm_Decreasing_noinf$ci.lb)
ci_upper_noinf <- plogis(glmm_Decreasing_noinf$ci.ub)

# Create table
sens_infl_summary <- data.frame(
  Analysis = "Excluding Influential Studies",
  Pooled_Prevalence = round(prev_noinf, 3),
  CI_Lower = round(ci_lower_noinf, 3),
  CI_Upper = round(ci_upper_noinf, 3),
  Studies_included = glmm_Decreasing_noinf$k
)

knitr::kable(sens_infl_summary, caption = "Sensitivity Analysis: Decreasing Trajectory (Excluding Influential Studies)")
```

### Use `glmer` estimate

Use with the glmer\`-based intercept estimate:

```{r Decreasing-glmer-summary}

# Extract fixed effect estimate
est_glmer_Decreasing <- fixef(model_glmer_Decreasing)["(Intercept)"]
se_glmer_Decreasing <- sqrt(vcov(model_glmer_Decreasing)["(Intercept)", "(Intercept)"])

ci_lower_glmer_Decreasing <- est_glmer_Decreasing - 1.96 * se_glmer_Decreasing
ci_upper_glmer_Decreasing <- est_glmer_Decreasing + 1.96 * se_glmer_Decreasing

# Number of studies included in glmer model
n_glmer_studies_Decreasing <- length(ranef(model_glmer_Decreasing)$Study[[1]])

# Back-transform to probability scale
glmer_summary_Decreasing <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_Decreasing), 3),
  CI_Lower = round(plogis(ci_lower_glmer_Decreasing), 3),
  CI_Upper = round(plogis(ci_upper_glmer_Decreasing), 3),
  Studies_included = n_glmer_studies_Decreasing
)

knitr::kable(glmer_summary_Decreasing, caption = "Decreasing Trajectory Estimate from glmer Model")
```

## âš ï¸ Increasing Symptoms Trajectory

We estimate the pooled prevalence of the Increasing trajectory using a Generalized Linear Mixed Model (GLMM).

### Descriptives

```{r Increasing-descriptives}
# Copy the data
df_Increasing <- df_moderation

# Ensure Increasing_percentage is numeric
df_Increasing$Increasing_percentage <- as.numeric(df_Increasing$Increasing_percentage)


# Calculate Increasing_n
df_Increasing$Increasing_n <- round((df_Increasing$Increasing_percentage / 100) * df_Increasing$Sample_Size)

# Total number of individuals classified as Increasing
total_Increasing_n <- sum(df_Increasing$Increasing_n, na.rm = TRUE)

# Number of unique samples with Increasing data
unique_Increasing_studies <- length(unique(df_Increasing$Study[!is.na(df_Increasing$Increasing_percentage)]))

# Display as a table
Increasing_info <- data.frame(
  Description = c("Total number of individuals in Increasing trajectory", "Number of unique studies with Increasing data"),
  Value = format(c(total_Increasing_n, unique_Increasing_studies), big.mark = ",")
)

knitr::kable(Increasing_info, caption = "Descriptive Statistics for Increasing Trajectory")
```

### Generic GLMM

```{r Increasing-glmm}

# Apply continuity correction for 0% and 100%
extreme_Increasing <- which(
  !is.na(df_Increasing$Increasing_n) &
  !is.na(df_Increasing$Sample_Size) &
  (df_Increasing$Increasing_n == 0 | df_Increasing$Increasing_n == df_Increasing$Sample_Size)
)

df_Increasing$Increasing_n[extreme_Increasing] <- df_Increasing$Increasing_n[extreme_Increasing] + 0.5
df_Increasing$Sample_Size[extreme_Increasing] <- df_Increasing$Sample_Size[extreme_Increasing] + 1

# Enforce xi <= ni - 0.5 to prevent metafor errors
df_Increasing$Increasing_n <- pmin(df_Increasing$Increasing_n, df_Increasing$Sample_Size - 0.5)

# Fit GLMM with rma.glmm
glmm_Increasing <- rma.glmm(
  measure = "PLO",
  xi = df_Increasing$Increasing_n,
  ni = df_Increasing$Sample_Size,
  data = df_Increasing
)

# Extract estimates
Increasing_prev <- plogis(coef(glmm_Increasing))
Increasing_ci_lower <- plogis(glmm_Increasing$ci.lb)
Increasing_ci_upper <- plogis(glmm_Increasing$ci.ub)

# Number of samples included
n_Increasing_studies <- glmm_Increasing$k

# Create summary table
Increasing_summary <- data.frame(
  Trajectory = "Increasing",
  Pooled_Prevalence = round(Increasing_prev, 3),
  CI_Lower = round(Increasing_ci_lower, 3),
  CI_Upper = round(Increasing_ci_upper, 3),
  Studies_included = n_Increasing_studies
)

knitr::kable(Increasing_summary, caption = "Pooled Prevalence Estimate for Increasing Trajectory (GLMM)")

```

### Sensitivity Analysis 1

Exclude Samples with â‰¥90% Decreasing Proportions

```{r Increasing-sens1}
# Compute Increasing proportion
df_Increasing$Increasing_Proportion <- df_Increasing$Increasing_n / df_Increasing$Sample_Size

# Subset: Exclude samples with Increasing â‰¥90%
df_Increasing_sens1 <- df_Increasing %>%
  filter(Increasing_Proportion < 0.90)

# Fit GLMM
glmm_Increasing_sens1 <- rma.glmm(
  measure = "PLO",
  xi = Increasing_n,
  ni = Sample_Size,
  data = df_Increasing_sens1
)

# Extract results
sens1_prev <- plogis(coef(glmm_Increasing_sens1))
sens1_ci_lower <- plogis(glmm_Increasing_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_Increasing_sens1$ci.ub)

# Create table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude â‰¥90% Increasing",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  Studies_included = glmm_Increasing_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Increasing Trajectory (Excl. â‰¥90%)")
```

### Sensitivity Analysis 2

Include Only Samples with N \> 999

```{r Increasing-sens2}
# Subset: Studies with Sample_Size > 999
df_Increasing_sens2 <- df_Increasing %>%
  filter(Sample_Size > 999)

# Fit GLMM
glmm_Increasing_sens2 <- rma.glmm(
  measure = "PLO",
  xi = Increasing_n,
  ni = Sample_Size,
  data = df_Increasing_sens2
)

# Extract results
sens2_prev <- plogis(coef(glmm_Increasing_sens2))
sens2_ci_lower <- plogis(glmm_Increasing_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_Increasing_sens2$ci.ub)

# Create table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  Studies_included = glmm_Increasing_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Increasing Trajectory (Samples >999)")


```

### Sensitivity Analysis 3

Exclude Influential Studies

```{r Increasing-influential}
# Fit glmer model
model_glmer_Increasing <- glmer(cbind(round(Increasing_n), Sample_Size - round(Increasing_n)) ~ 1 + (1 | Study),
                               data = df_Increasing, family = binomial)

# Influence analysis
infl_Increasing <- influence(model_glmer_Increasing, group = "Study")
cooks_Increasing <- cooks.distance(infl_Increasing)

# Define threshold
threshold_Increasing <- 4 / length(cooks_Increasing)

# Identify influential studies
influential_idx_Increasing <- which(cooks_Increasing > threshold_Increasing)
influential_studies_Increasing <- df_Increasing$Study[influential_idx_Increasing]

# Display table
influential_table_Increasing <- data.frame(
  Study_ID = influential_studies_Increasing,
  Cooks_Distance = round(cooks_Increasing[influential_idx_Increasing], 3)
)

# Plot Cook's distances
plot(cooks_Increasing, type = "h", lwd = 2,
     main = "Cook's Distance per Study (Increasing)",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold_Increasing, col = "red", lty = 2)

knitr::kable(influential_table_Increasing, caption = "Influential Studies in Increasing Trajectory (Cook's Distance)")


```

Excluding Influential Studies

```{r Increasing-sens-infl}
# Exclude influential studies
df_Increasing_noinf <- df_Increasing %>%
  filter(!Study %in% influential_studies_Increasing)

# Fit GLMM after exclusion
glmm_Increasing_noinf <- rma.glmm(
  measure = "PLO",
  xi = Increasing_n,
  ni = Sample_Size,
  data = df_Increasing_noinf
)

# Extract results
prev_noinf <- plogis(coef(glmm_Increasing_noinf))
ci_lower_noinf <- plogis(glmm_Increasing_noinf$ci.lb)
ci_upper_noinf <- plogis(glmm_Increasing_noinf$ci.ub)

# Create table
sens_infl_summary_Increasing <- data.frame(
  Analysis = "Excluding Influential Studies",
  Pooled_Prevalence = round(prev_noinf, 3),
  CI_Lower = round(ci_lower_noinf, 3),
  CI_Upper = round(ci_upper_noinf, 3),
  Studies_included = glmm_Increasing_noinf$k
)

knitr::kable(sens_infl_summary_Increasing, caption = "Sensitivity Analysis: Increasing Trajectory (Excluding Influential Studies)")

```

### Use `glmer` estimate

Use with the glmer\`-based intercept estimate:

```{r Increasing-glmer-summary}

# Extract fixed effect estimate
est_glmer_Increasing <- fixef(model_glmer_Increasing)["(Intercept)"]
se_glmer_Increasing <- sqrt(vcov(model_glmer_Increasing)["(Intercept)", "(Intercept)"])

ci_lower_glmer_Increasing <- est_glmer_Increasing - 1.96 * se_glmer_Increasing
ci_upper_glmer_Increasing <- est_glmer_Increasing + 1.96 * se_glmer_Increasing

# Number of studies included in glmer model
n_glmer_studies_Increasing <- length(ranef(model_glmer_Increasing)$Study[[1]])

# Back-transform to probability scale
glmer_summary_Increasing <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_Increasing), 3),
  CI_Lower = round(plogis(ci_lower_glmer_Increasing), 3),
  CI_Upper = round(plogis(ci_upper_glmer_Increasing), 3),
  Studies_included = n_glmer_studies_Increasing
)

knitr::kable(glmer_summary_Increasing, caption = "Increasing Trajectory Estimate from glmer Model")
```

## ðŸ©¸ High Symptoms Trajectory

We estimate the pooled prevalence of the High trajectory using a Generalized Linear Mixed Model (GLMM).

### Descriptives

```{r High-descriptives}

# Copy the data
df_High <- df_moderation

# Ensure High_percentage is numeric
df_High$High_percentage <- as.numeric(df_High$High_percentage)

# Calculate High_n
df_High$High_n <- round((df_High$High_percentage / 100) * df_High$Sample_Size)


# Total number of individuals classified as High
total_High_n <- sum(df_High$High_n, na.rm = TRUE)

# Number of unique samples with High data
unique_High_studies <- length(unique(df_High$Study[!is.na(df_High$High_percentage)]))

# Display as a table
High_info <- data.frame(
  Description = c("Total number of individuals in High trajectory", "Number of unique studies with High data"),
  Value = format(c(total_High_n, unique_High_studies), big.mark = ",")
)

knitr::kable(High_info, caption = "Descriptive Statistics for High Trajectory")

```

### Generic GLMM

```{r High-glmm}


# Apply continuity correction for 0% and 100%
extreme_High <- which(
  !is.na(df_High$High_n) &
  !is.na(df_High$Sample_Size) &
  (df_High$High_n == 0 | df_High$High_n == df_High$Sample_Size)
)

df_High$High_n[extreme_High] <- df_High$High_n[extreme_High] + 0.5
df_High$Sample_Size[extreme_High] <- df_High$Sample_Size[extreme_High] + 1

# Enforce xi â‰¤ ni - 0.5 to prevent metafor errors
df_High$High_n <- pmin(df_High$High_n, df_High$Sample_Size - 0.5)

# Fit GLMM with rma.glmm
glmm_High <- rma.glmm(
  measure = "PLO",
  xi = df_High$High_n,
  ni = df_High$Sample_Size,
  data = df_High
)

# Extract estimates
High_prev <- plogis(coef(glmm_High))
High_ci_lower <- plogis(glmm_High$ci.lb)
High_ci_upper <- plogis(glmm_High$ci.ub)

# Number of samples included
n_High_studies <- glmm_High$k

# Create summary table
High_summary <- data.frame(
  Trajectory = "High",
  Pooled_Prevalence = round(High_prev, 3),
  CI_Lower = round(High_ci_lower, 3),
  CI_Upper = round(High_ci_upper, 3),
  Studies_included = n_High_studies
)

knitr::kable(High_summary, caption = "Pooled Prevalence Estimate for High Trajectory (GLMM)")

```

### Sensitivity Analysis 1

Exclude Samples with â‰¥90% Decreasing Proportions

```{r High-sens1}
# Compute High proportion
df_High$High_Proportion <- df_High$High_n / df_High$Sample_Size

# Subset: Exclude samples with High â‰¥90%
df_High_sens1 <- df_High %>%
  filter(High_Proportion < 0.90)

# Fit GLMM
glmm_High_sens1 <- rma.glmm(
  measure = "PLO",
  xi = High_n,
  ni = Sample_Size,
  data = df_High_sens1
)

# Extract results
sens1_prev <- plogis(coef(glmm_High_sens1))
sens1_ci_lower <- plogis(glmm_High_sens1$ci.lb)
sens1_ci_upper <- plogis(glmm_High_sens1$ci.ub)

# Create table
sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude â‰¥90% High",
  Pooled_Prevalence = round(sens1_prev, 3),
  CI_Lower = round(sens1_ci_lower, 3),
  CI_Upper = round(sens1_ci_upper, 3),
  Studies_included = glmm_High_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: High Trajectory (Excl. â‰¥90%)")
```

### Sensitivity Analysis 2

Include Only Samples with N \> 999

```{r High-sens2}
# Subset: Samples with Sample_Size > 999
df_High_sens2 <- df_High %>%
  filter(Sample_Size > 999)

# Fit GLMM
glmm_High_sens2 <- rma.glmm(
  measure = "PLO",
  xi = High_n,
  ni = Sample_Size,
  data = df_High_sens2
)

# Extract results
sens2_prev <- plogis(coef(glmm_High_sens2))
sens2_ci_lower <- plogis(glmm_High_sens2$ci.lb)
sens2_ci_upper <- plogis(glmm_High_sens2$ci.ub)

# Create table
sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(sens2_prev, 3),
  CI_Lower = round(sens2_ci_lower, 3),
  CI_Upper = round(sens2_ci_upper, 3),
  Studies_included = glmm_High_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: High Trajectory (Samples >999)")

```

### Sensitivity Analysis 3

Exclude Influential Studies

```{r High-influential}
# Fit glmer model
model_glmer_High <- glmer(cbind(round(High_n), Sample_Size - round(High_n)) ~ 1 + (1 | Study),
                              data = df_High, family = binomial)

# Influence analysis
infl_High <- influence(model_glmer_High, group = "Study")
cooks_High <- cooks.distance(infl_High)

# Define threshold
threshold_High <- 4 / length(cooks_High)

# Plot Cook's distances
plot(cooks_High, type = "h", lwd = 2,
     main = "Cook's Distance per Study (High)",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold_High, col = "red", lty = 2)

# Identify influential studies
influential_idx_High <- which(cooks_High > threshold_High)
influential_studies_High <- df_High$Study[influential_idx_High]

# Display table
influential_table_High <- data.frame(
  Study_ID = influential_studies_High,
  Cooks_Distance = round(cooks_High[influential_idx_High], 3)
)

knitr::kable(influential_table_High, caption = "Influential Studies in High Trajectory (Cook's Distance)")

```

Exclude Influential Studies

```{r High-sens-infl}
# Exclude influential studies
df_High_noinf <- df_High %>%
  filter(!Study %in% influential_studies_High)

# Fit GLMM after exclusion
glmm_High_noinf <- rma.glmm(
  measure = "PLO",
  xi = High_n,
  ni = Sample_Size,
  data = df_High_noinf
)

# Extract results
prev_noinf <- plogis(coef(glmm_High_noinf))
ci_lower_noinf <- plogis(glmm_High_noinf$ci.lb)
ci_upper_noinf <- plogis(glmm_High_noinf$ci.ub)

# Create table
sens_infl_summary_High <- data.frame(
  Analysis = "Excluding Influential Studies",
  Pooled_Prevalence = round(prev_noinf, 3),
  CI_Lower = round(ci_lower_noinf, 3),
  CI_Upper = round(ci_upper_noinf, 3),
  Studies_included = glmm_High_noinf$k
)

knitr::kable(sens_infl_summary_High, caption = "Sensitivity Analysis: High Trajectory (Excluding Influential Studies)")

```

### Use `glmer`estimate

Use with the glmer\`-based intercept estimate:

```{r High-glmer-summary}

# Extract fixed effect estimate
est_glmer_High <- fixef(model_glmer_High)["(Intercept)"]
se_glmer_High <- sqrt(vcov(model_glmer_High)["(Intercept)", "(Intercept)"])

ci_lower_glmer_High <- est_glmer_High - 1.96 * se_glmer_High
ci_upper_glmer_High <- est_glmer_High + 1.96 * se_glmer_High

# Number of studies included in glmer model
n_glmer_studies_High <- length(ranef(model_glmer_High)$Study[[1]])

# Back-transform to probability scale
glmer_summary_High <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer_High), 3),
  CI_Lower = round(plogis(ci_lower_glmer_High), 3),
  CI_Upper = round(plogis(ci_upper_glmer_High), 3),
  Studies_included = n_glmer_studies_High
)

knitr::kable(glmer_summary_High, caption = "High Trajectory Estimate from glmer Model")

```

## ðŸŒ— Moderate Symptoms Trajectory

We estimate the pooled prevalence of the Moderate trajectory using a Generalized Linear Mixed Model (GLMM)

### Descriptives

```{r moderate-descriptives}

# Copy the data
df_moderate <- df_moderation


# Ensure Moderate_percentage is numeric
df_moderate$Moderate_percentage <- as.numeric(df_moderate$Moderate_percentage)

# Calculate Moderate_n
df_moderate$Moderate_n <- round((df_moderate$Moderate_percentage / 100) * df_moderate$Sample_Size)

total_Moderate_n <- sum(df_moderate$Moderate_n, na.rm = TRUE)

# Number of unique samples with Moderate data

unique_Moderate_studies <- length(unique(df_moderate$Study[!is.na(df_moderate$Moderate_percentage)]))

# Total number of individuals classified as Moderate


Moderate_info <- data.frame(
  Description = c("Total number of individuals in Moderate trajectory", "Number of unique studies with Moderate data"),
  Value = format(c(total_Moderate_n, unique_Moderate_studies), big.mark = ",")
)

knitr::kable(Moderate_info, caption = "Descriptive Statistics for Moderate Trajectory")
```

### Generic GLMM

```{r moderate-glmm}



# Apply continuity correction for 0% and 100%
extreme_Moderate <- which(!is.na(df_moderate$Moderate_n) &
                           !is.na(df_moderate$Sample_Size) &
                           (df_moderate$Moderate_n == 0 | df_moderate$Moderate_n == df_moderate$Sample_Size))

df_moderate$Moderate_n[extreme_Moderate] <- df_moderate$Moderate_n[extreme_Moderate] + 0.5
df_moderate$Sample_Size[extreme_Moderate] <- df_moderate$Sample_Size[extreme_Moderate] + 1

# Prevent xi > ni by enforcing xi <= ni - 0.5
df_moderate$Moderate_n <- pmin(df_moderate$Moderate_n, df_moderate$Sample_Size - 0.5)


# Fit GLMM
glmm_Moderate <- rma.glmm(measure = "PLO",
                           xi = df_moderate$Moderate_n,
                           ni = df_moderate$Sample_Size,
                           data = df_moderate)

# Extract estimates
Moderate_prev <- plogis(coef(glmm_Moderate))
Moderate_ci_lower <- plogis(glmm_Moderate$ci.lb)
Moderate_ci_upper <- plogis(glmm_Moderate$ci.ub)

# Create summary table
Moderate_summary <- data.frame(
  Trajectory = "Moderate",
  Pooled_Prevalence = round(Moderate_prev, 3),
  CI_Lower = round(Moderate_ci_lower, 3),
  CI_Upper = round(Moderate_ci_upper, 3),
  Studies_included = glmm_Moderate$k
)

knitr::kable(Moderate_summary, caption = "Pooled Prevalence Estimate for Moderate  Trajectory (GLMM)")
```

### Sensitivity Analysis 1

Exclude Samples with â‰¥90% Prevalence

```{r moderate-sens1}
df_moderate$Moderate_Proportion <- df_moderate$Moderate_n / df_moderate$Sample_Size

df_moderate_sens1 <- df_moderate %>%
  filter(Moderate_Proportion < 0.90)

glmm_Moderate_sens1 <- rma.glmm(measure = "PLO",
                                 xi = Moderate_n,
                                 ni = Sample_Size,
                                 data = df_moderate_sens1)

sens1_summary <- data.frame(
  Analysis = "Sensitivity 1: Exclude â‰¥90% Moderate ",
  Pooled_Prevalence = round(plogis(coef(glmm_Moderate_sens1)), 3),
  CI_Lower = round(plogis(glmm_Moderate_sens1$ci.lb), 3),
  CI_Upper = round(plogis(glmm_Moderate_sens1$ci.ub), 3),
  Studies_included = glmm_Moderate_sens1$k
)

knitr::kable(sens1_summary, caption = "Sensitivity Analysis 1: Moderate  Trajectory (Excl. â‰¥90%)")
```

### Sensitivity Analysis 2

Only Large Samples (N \> 999)

```{r moderate-sens2}
df_moderate_sens2 <- df_moderate %>%
  filter(Sample_Size > 999)

glmm_Moderate_sens2 <- rma.glmm(measure = "PLO",
                                 xi = Moderate_n,
                                 ni = Sample_Size,
                                 data = df_moderate_sens2)

sens2_summary <- data.frame(
  Analysis = "Sensitivity 2: N > 999",
  Pooled_Prevalence = round(plogis(coef(glmm_Moderate_sens2)), 3),
  CI_Lower = round(plogis(glmm_Moderate_sens2$ci.lb), 3),
  CI_Upper = round(plogis(glmm_Moderate_sens2$ci.ub), 3),
  Studies_included = glmm_Moderate_sens2$k
)

knitr::kable(sens2_summary, caption = "Sensitivity Analysis 2: Moderate  Trajectory (Samples >999)")
```

### Sensitivity Analysis 3

Exclude Influential Studies

```{r moderate-influential}
model_glmer_Moderate <- glmer(cbind(round(Moderate_n), Sample_Size - round(Moderate_n)) ~ 1 + (1 | Study),
                               data = df_moderate, family = binomial)

infl_Moderate <- influence(model_glmer_Moderate, group = "Study")
cooks_Moderate <- cooks.distance(infl_Moderate)

threshold_Moderate <- 4 / length(cooks_Moderate)

plot(cooks_Moderate, type = "h", lwd = 2,
     main = "Cook's Distance per Study (Moderate)",
     ylab = "Cook's Distance", xlab = "Study Index")
abline(h = threshold_Moderate, col = "red", lty = 2)

influential_idx <- which(cooks_Moderate > threshold_Moderate)
influential_studies <- df_moderate$Study[influential_idx]

influential_table <- data.frame(
  Study_ID = influential_studies,
  Cooks_Distance = round(cooks_Moderate[influential_idx], 3)
)

knitr::kable(influential_table, caption = "Influential Studies in Moderate Trajectory (Cook's Distance)")
```

Refit Model Excluding Influential Studies

```{r moderate-sens-infl}
df_Moderate_noinf <- df_moderate %>% filter(!Study %in% influential_studies)

glmm_Moderate_noinf <- rma.glmm(measure = "PLO",
                       xi = Moderate_n,
                       ni = Sample_Size,
                       data = df_Moderate_noinf)

sens_infl_summary <- data.frame(
  Analysis = "Excluding Influential Studies",
  Pooled_Prevalence = round(plogis(coef(glmm_Moderate_noinf)), 3),
  CI_Lower = round(plogis(glmm_Moderate_noinf$ci.lb), 3),
  CI_Upper = round(plogis(glmm_Moderate_noinf$ci.ub), 3),
  Studies_included = glmm_Moderate_noinf$k
)

knitr::kable(sens_infl_summary, caption = "Sensitivity Analysis: Moderate Trajectory (Excluding Influential Studies)")
```

### Use `glmer`estimate

```{r moderate-glmer-summary}
est_glmer <- fixef(model_glmer_Moderate)["(Intercept)"]
se_glmer <- sqrt(vcov(model_glmer_Moderate)["(Intercept)", "(Intercept)"])

ci_lower <- est_glmer - 1.96 * se_glmer
ci_upper <- est_glmer + 1.96 * se_glmer

n_glmer_studies <- length(ranef(model_glmer_Moderate)$Study[[1]])

final_summary <- data.frame(
  Model = "glmer (binomial, random intercept)",
  Pooled_Prevalence = round(plogis(est_glmer), 3),
  CI_Lower = round(plogis(ci_lower), 3),
  CI_Upper = round(plogis(ci_upper), 3),
  Studies_included = n_glmer_studies
)

knitr::kable(final_summary, caption = "Moderate  Trajectory Estimate from glmer Model")
```

# Tables for Paper
## Prevalences Summary Table

```{r prevalences-summary-table}
# Set trajectory order & define fitted models
traj_order <- c("Low","Decreasing","Increasing","High","Moderate")

fits <- list(
  Low        = glmm_Low,
  Decreasing = glmm_Decreasing,
  Increasing = glmm_Increasing,
  High       = glmm_High,
  Moderate   = glmm_Moderate
)

# define number of people per trajectory with percentage
trajectory_counts <- tibble(
  trajectory = factor(traj_order, levels = traj_order),
  N = c(
    total_Low_n,
    total_Decreasing_n,
    total_Increasing_n,
    total_High_n,
    total_Moderate_n
  )
) |>
  mutate(pct = round(100 * N / sum(N), 1))

# define k (unique samples) per trajectory
k_vec <- c(
  Low        = as.integer(unique_Low_studies),
  Decreasing = as.integer(unique_Decreasing_studies),
  Increasing = as.integer(unique_Increasing_studies),
  High       = as.integer(unique_High_studies),
  Moderate   = as.integer(unique_Moderate_studies)
)

# build one row per trajectory from the GLMM fit
row_from <- function(name, fit) {
  pr <- predict(fit, transf = transf.ilogit)
  tibble(
    trajectory = name,
    rel_prev   = sprintf("%.1f%%", pr$pred * 100), # Relative Prevalence Column
    ci_95      = sprintf("%.1f to %.1f%%", pr$ci.lb * 100, pr$ci.ub * 100), # 95% Confidence Interval
    k          = k_vec[[name]], # unique studies
    N_pct      = sprintf(
                   "%s (%.1f%%)",
                   format(trajectory_counts$N[trajectory_counts$trajectory == name], big.mark = ","),
                   trajectory_counts$pct[trajectory_counts$trajectory == name]
                 ), # number of people in trajectory & % of total number of people
    logit      = round(unname(coef(fit)[1]), 2), # logit estimate
    tau2       = round(as.numeric(fit$tau2), 2) # between-study heterogeneity
  )
}

table1_df <- bind_rows(lapply(traj_order, function(nm) row_from(nm, fits[[nm]])))

# Render table
tbl1 <- table1_df |>
  gt() |>
  cols_label(
    trajectory = "PTSD Symptom Trajectory",
    rel_prev   = "Relative Prevalence",
    ci_95      = "95% (CI)",
    k          = html("<em>k</em>"),
    N_pct      = "N (%)",
    logit      = "Logit Estimate",
    tau2       = html("<em>&tau;<sup>2</sup></em>")
  ) |>
  cols_align("left",   columns = trajectory) |>
  cols_align("center", columns = c(rel_prev, ci_95, k, N_pct, logit, tau2)) |>
  fmt_integer(columns = k, use_seps = TRUE) |>
  fmt_number(columns = c(logit, tau2), decimals = 2) |>
  opt_row_striping() |>
  tab_style(
    style = cell_text(size = px(11), font = "Times New Roman"),
    locations = cells_source_notes()
  ) |>
  tab_options(
    table.width = pct(100),
    data_row.padding = px(6),
    table.font.size = px(13),
    table.font.names = "Times New Roman"
  ) |>
  tab_source_note(
    source_note = html("<em>Note.</em> <em>k</em> represents the number of unique samples. Logit estimate reflects pooled prevalence on a log-odds scale. <em>&tau;<sup>2</sup></em> indicates between-study heterogeneity; values &gt; 0.50 suggest substantial heterogeneity.")
  ) |>
  tab_style(
    style = cell_text(size = px(11)),       
    locations = cells_source_notes()
  ) |>
  opt_table_font(
    font = list(
      google_font("Times New Roman"),
      default_fonts()     
    )
  )

print(tbl1)

# Also export a Word-friendly file alongside the HTML
gt::gtsave(tbl1, "prevalences-summary-table.png", vwidth = 700)
gt::gtsave(tbl1, "prevalences-summary-table.rtf")
```

## Sensitivity Analyses Summary

```{r sensitivity-summary-table}
# build one row per trajectory from the GLMM fit
mk_row <- function(fit, label) {
  prev  <- plogis(as.numeric(coef(fit)))
  ci_lb <- plogis(as.numeric(fit$ci.lb))
  ci_ub <- plogis(as.numeric(fit$ci.ub))
  tibble(
    Analysis = label,
    k        = as.integer(fit$k),
    `Relative Prevalence (95% CI)` =
      sprintf("%.1f%% [%.1f%% â€“ %.1f%%]", 100 * prev, 100 * ci_lb, 100 * ci_ub),
    `Logit Estimate` = as.numeric(coef(fit)),
    tau2             = if (!is.null(fit$tau2)) as.numeric(fit$tau2) else NA_real_
  )
}

# define components of the table (trajectory names & analysis names)
trajectory_map <- c(
  "Low"        = "Low symptoms",
  "Decreasing" = "Decreasing symptoms",
  "Increasing" = "Increasing symptoms",
  "High"       = "High symptoms",
  "Moderate"   = "Moderate symptoms"
)

analysis_map <- c(
  "Main"  = "Main analysis (all studies)",
  "sens1" = "Excluding studies with extreme prevalence (â‰¥90%)",
  "sens2" = "Including only large-sample studies (N â‰¥ 1000)",
  "noinf" = "Excluding influential studies"
)

# Build a spec table combining every trajectory with every analysis 
all_specs <- crossing(
  trajectory_stub = names(trajectory_map),
  analysis_stub   = names(analysis_map)
) |>
# Add derived columns to the Cartesian product above  
mutate(
    object_name = if_else(
      analysis_stub == "Main",
      paste0("glmm_", trajectory_stub),
      paste0("glmm_", trajectory_stub, "_", analysis_stub)
    ),
    # Look up human-readable trajectory and analysis label by name.
    # indexing by _stub returns the corresponding value.
    Trajectory = trajectory_map[trajectory_stub],
    Analysis   = analysis_map[analysis_stub]
  )

# build the table
big_tbl <- pmap_dfr(
  all_specs,
  function(object_name, Trajectory, Analysis, ...) {
    fit <- tryCatch(get(object_name, envir = .GlobalEnv), error = \(e) NULL)
    if (!is.null(fit)) {
      mk_row(fit, Analysis) |>
        mutate(Trajectory = Trajectory, .before = 1)
    }
  }
)

# enfore trajectory order
trajectory_order <- c(
  "Low symptoms",
  "Decreasing symptoms",
  "Increasing symptoms",
  "High symptoms",
  "Moderate symptoms"
)

big_tbl_ordered <- big_tbl |>
  mutate(Trajectory = factor(Trajectory, levels = trajectory_order)) |>
  arrange(Trajectory)

# create gt table
gt_all <-
  gt(big_tbl_ordered, groupname_col = "Trajectory") |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_row_groups() # group analyses by trajectory
  ) |>
  cols_label(
    Analysis = "Analysis",
    k = html("<em>k</em>"),
    `Relative Prevalence (95% CI)` = "Relative Prevalence (95% CI)",
    `Logit Estimate` = "Logit Estimate",
    tau2 = html("<em>&tau;<sup>2</sup></em>")
  ) |>
  fmt_number(columns = c(`Logit Estimate`, tau2), decimals = 2) |>
  cols_align(align = "left", columns = Analysis) |>
  cols_align(align = "center", columns = c(k, `Relative Prevalence (95% CI)`, `Logit Estimate`, tau2)) |>
  opt_row_striping() |>
  tab_options(
    table.font.names = c("Times New Roman", "Times", "serif"),  # force times new roman font in outputs
    data_row.padding = px(6),
    column_labels.font.weight = "bold",
    row_group.as_column = FALSE,
    table.font.size = px(13)
  ) |>
  # smaller notes/footnotes
  tab_style(
    style = cell_text(size = px(11), font = "Times New Roman"),
    locations = list(cells_source_notes(), cells_footnotes())
  ) |>
  tab_source_note(md(
    "<em>Note.</em> <em>k</em> represents the number of unique samples. Logit estimate reflects pooled prevalence on a log-odds scale. <em>&tau;<sup>2</sup></em> indicates between-study heterogeneity; values &gt; 0.50 suggest substantial heterogeneity."
  ))


# Auto-notes for excluded influential papers using Study variable ("Author, Year")
.get_model_data <- function(fit) { # recovers the original data frame used to fit a model
  d <- tryCatch({
    cd <- fit$call$data
    if (is.symbol(cd)) get(as.character(cd), envir = .GlobalEnv)
    else if (is.character(cd)) get(cd, envir = .GlobalEnv)
    else NULL
  }, error = function(e) NULL)
  if (is.null(d)) tryCatch(model.frame(fit), error = function(e) NULL) else d
}

.used_rows <- function(fit, n_df) { # Figures out which rows of the original data were kept when the model was fit.
  if (!is.null(fit$not.na) && length(fit$not.na) == n_df) which(fit$not.na) else seq_len(n_df)
}

.labels_from_study <- function(fit) { # Gets the modelâ€™s data via .get_model_data(fit) and uses .used_rows() to take only rows actually used in the fit.
  df <- .get_model_data(fit)
  if (is.null(df)) return(character(0))
  nm <- if ("Study" %in% names(df)) "Study" else if ("study" %in% names(df)) "study" else stop("Column 'Study' not found.")
  idx <- .used_rows(fit, nrow(df))
  as.character(df[[nm]])[idx]
}

.excluded_study_labels <- function(traj_stub) { # Gets trimmed study labels frommain model and noinf model using .labels_from_study()
  main_fit  <- tryCatch(get(paste0("glmm_", traj_stub), envir = .GlobalEnv), error = function(e) NULL)
  noinf_fit <- tryCatch(get(paste0("glmm_", traj_stub, "_noinf"), envir = .GlobalEnv), error = function(e) NULL)
  if (is.null(main_fit)) return(character(0))
  main_labs <- trimws(.labels_from_study(main_fit))
  if (!is.null(noinf_fit)) {
    noinf_labs <- trimws(.labels_from_study(noinf_fit))
    setdiff(main_labs, noinf_labs)
  } else {
    character(0)  # no _noinf model => no exclusions to show
  }
}

format_note <- function(vec) {
  if (length(vec) == 0) "No influential studies excluded."
  else paste0("Excluded influential studies: ", paste(vec, collapse = "; "))
}

# Build notes per displayed group label:
# Name the resulting list with the display labels from trajectory_map
excluded_by_group <- setNames(
  lapply(names(trajectory_map), .excluded_study_labels),
  nm = unname(trajectory_map[names(trajectory_map)])
)
# apply format_note()
excluded_notes <- lapply(excluded_by_group, format_note)

# Attach footnote to the â€œExcluding influential studiesâ€ row within each group
gt_all <- purrr::reduce(
  names(excluded_notes),
  .init = gt_all,
  .f = function(gt_obj, grp_label) {
    tab_footnote(
      gt_obj,
      footnote  = excluded_notes[[grp_label]],
      locations = cells_body(
        columns = "Analysis",
        rows = (Trajectory == grp_label) & (Analysis == "Excluding influential studies")
      )
    )
  }
)

gt_all

# Export to rtf and png to use in paper
gt::gtsave(gt_all, "prevalences-sensitivity-table.rtf")
gt::gtsave(gt_all, "prevalences-sensitivity-table.png", vwidth = 700) 

```




